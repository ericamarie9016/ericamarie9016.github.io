[
  {
    "objectID": "travel.html",
    "href": "travel.html",
    "title": "Travels",
    "section": "",
    "text": "* This map depicts the different places I have lived and traveled along with some highlights of my professional, volunteer, and personal experiences."
  },
  {
    "objectID": "posts/2022-12-9-stats-proj/index.html",
    "href": "posts/2022-12-9-stats-proj/index.html",
    "title": "Statistics Blog Post",
    "section": "",
    "text": "Code\n# All libraries here\nlibrary(tidyverse)  # Data manipulization and visualization\nlibrary(leaflet)    # Create interactive map\nlibrary(lubridate)  # Working with dates and times\nlibrary(ggpubr)     # Combines ggplots\nlibrary(here)       # Set file location\nlibrary(readr)      # Reading files into data frames\nlibrary(gt)         # Create tables\nlibrary(tufte)      # Implementing Tufte-style graphics\nlibrary(feasts)     # Time series analysis\nlibrary(knitr)      # Creates nice tables\n\n\n\nIntroduction\nI currently enjoy living near the beach in Santa Barbara, but this comes with inadvertent ecological impacts as coastal development and high recreational use increase pollutants and impacts the coastal ecosystem. A major question arises: Does human development on land cause significant change in the species abundance of coastal waters? The aim of this project is to provide an understanding of sensitivities select species have to developed coastlines.\nData was used from the Santa Barbara Coastal Long Term Ecological Research dataset “Settlement of urchins and other invertebrates, ongoing since 1990.” This project collects counts on six taxonomic groups through regular summer surveys. The survey locations are throughout southern California with one north of San Francisco. Each location is a pier, the counts are collected from the end of the pier in the littoral zone. The littoral zone is below the low-tide level and experiences the effects of tidal and longshore currents, and breaking waves.\nThree of the piers are off protected coastal regions, the rest are populated and developed piers. The protected regions are all parks with minimal development and a marine conservation area protecting surrounding water: Gaviota Beach Pier with Kashtayit State Marine Conservation Area, Point Cabrillo Pier with the Point Cabrillo State Marine Reserve, Anacapa Island Pier with a variety of marine protected areas surrounding the Channel Islands chain. The maps below display the survey locations, 3 protected and 5 unprotected.\n\nLocations of the LTER Survey Sites:\n\n\nCode\n# Create a data frame with the coordinates and protection status\nprotection_coordinates &lt;- data.frame(\n  lat = c(34.0085, 39.3142, 34.4645, 37.7523, 34.4152, 34.4133, 32.865, 35.1794),\n  lon = c(-119.4236, -123.7461, -120.2307, -122.5102, -119.8746, -119.6852, -117.2504, -120.7347),\n  protection = c(rep(\"Protected\", 3), rep(\"Unprotected\", 5)),\n  Fullname = c(\"Anacapa\", \"Point Cabrillo\", \"Gaviota\", \"Ocean Beach\", \"Ellwood Pier\", \"Stearns Wharf\", \"Scripps\", \"Avila Beach\")\n)\n\n# Create a leaflet map centered on California\nmap_sites &lt;- leaflet() %&gt;% \n  setView(lng = -119.4179, lat = 35.7783, zoom = 6) %&gt;% \n  addTiles()\n\n# Add locations to the map\nmap_sites &lt;- map_sites %&gt;%  \n  \n  # Protected points  \n  addCircleMarkers(\n    data = protection_coordinates[protection_coordinates$protection == \"Protected\", ], \n    lat = ~lat,             # use lat/long from dataframe\n    lng = ~lon, \n    color = \"blue\", \n    radius = 6,             # set the radius of the marker\n    popup = ~Fullname       # add the site name to the popup\n  ) %&gt;%  \n  \n  # Unprotected points\n  addCircleMarkers(\n    data = protection_coordinates[protection_coordinates$protection == \"Unprotected\", ], \n    lat = ~lat, \n    lng = ~lon, \n    color = \"darkorange\", \n    radius = 6, \n    popup = ~Fullname # add the site name to the popup\n  )\n\n# Add a legend to the map\nmap_sites &lt;- addLegend(\n  map = map_sites,\n  position = \"topright\",\n  title = \"Protection Status\",\n  colors = c(\"blue\", \"darkorange\"),\n  labels = c(\"Protected\", \"Unprotected\")\n)\n\n# Display the map\nmap_sites\n\n\n\n\n\n\n\n\n\nThis data set explores a variety of species spanning different ecological functions:\n\nArthropoda: Phylum of lobsters, crabs, and barnacles which have an exoskeleton made of chitin and risks mineralizing in calcium carbonate, a risk of ocean acidification.\nBivalvia: Class belonging to phylum Mollusca, includes filter feeders such as clams, oysters, and mussels.\nGastropoda: Class belonging to phylum Mollusca including snails, conch, abalone, limpets, and whelks with a range from scavengers, predators, herbivores, and parasites.\nOphiuroidea/Asterpodea: Brittle stars and sea stars respectively belonging to phylum Echinodermata, these are carnivores, filter feeders, and scavengers.\nS purpuratus: Pacific purple sea urchin, a sedentary species primarily feeding on algae.\nM franciscanus: Red sea urchins, which eat kept and seaweed.\nTotal Urchins: Combines counts of the previous two, S purpuratus and M franciscanus.\n\n\n\n\n\nData Collection and Tidying\n\nImporting and Creating Data Frames\nThis dataset was initially downloaded via DataOne website, link in the references. The metadata stated NA’s were written in the data as ‘-99999’, so I included this while importing the data.\n\n\nCode\n## Import LTER data \nUrchinsDF &lt;- read_csv(here(\"posts\", \"2022-12-9-stats-proj\", \"Invertebrate_Settlement_All_Years_20220722.csv\"), na = \"-99999\")\n\n\nThe protection data was not immediately available, I had to create a data frame with the values 1 for protected, 0 for unprotected. Protection was determined by the location of the piers. A pier is labeled unprotected if located in populated developed areas such as Ocean Beach, San Diego; protected piers are located in undeveloped parks surrounded by marine protected areas, including Channel Islands National Park and Point Cabrillo State Marine Reserve.\n\n\nCode\n## Create Protection data frame\nProtectionDF &lt;- data.frame(\n  Fullname = c(\"Anacapa\", \"Point Cabrillo\", \"Gaviota\", \"Ocean Beach\", \"Ellwood Pier\", \"Stearns Wharf\", \"Scripps\", \"Avila Beach\"),\n  SITE = c(\"ANACAPA\", \"FBPC\", \"GAVIOTA\", \"OCNBCH\", \"SBELL\", \"SBSTWRF\", \"SIO\", \"AVILA\"),\n  Protection = c(1, 1, 1, 0, 0, 0, 0, 0)\n)\n\nkable(ProtectionDF, n = 8)\n\n\n\n\n\nFullname\nSITE\nProtection\n\n\n\n\nAnacapa\nANACAPA\n1\n\n\nPoint Cabrillo\nFBPC\n1\n\n\nGaviota\nGAVIOTA\n1\n\n\nOcean Beach\nOCNBCH\n0\n\n\nEllwood Pier\nSBELL\n0\n\n\nStearns Wharf\nSBSTWRF\n0\n\n\nScripps\nSIO\n0\n\n\nAvila Beach\nAVILA\n0\n\n\n\n\n\n\n\nCombine Data Frames\nHere the two dataframes are joined so all of the survey rows are connected with a protection status based on location. Protection status is changed to be a binary TRUE/FALSE option, and the date class is changed. There are repeated surveys per day so these are collapsed down to one survey per day per site for my dataframe.\n\n\nCode\n## Join and select column fields\nUrchins &lt;- left_join(UrchinsDF, ProtectionDF, by = \"SITE\") |&gt; \n  select(SITE, Protection, DATE_RETRIEVED, ARTHROPODA, BIVALVIA, GASTROPODA, OPHIUROIDEA_ASTERPODEA, S_PURPURATUS, M_FRANCISCANUS, TOTAL_URCHINS)\n\n## Set as date class\nUrchins$DATE_RETRIEVED &lt;- lubridate::mdy(Urchins$DATE_RETRIEVED)\n\n## Collapse repeated surveys per day by site\nUrchins &lt;- Urchins |&gt; \n  group_by(SITE, DATE_RETRIEVED) |&gt;        # Combine by date, per site\n  summarise(ARTHROPODA = sum(ARTHROPODA, na.rm = TRUE),\n            BIVALVIA = sum(BIVALVIA, na.rm = TRUE),\n            GASTROPODA = sum(GASTROPODA, na.rm = TRUE),\n            OPHIUROIDEA_ASTERPODEA = sum(OPHIUROIDEA_ASTERPODEA, na.rm = TRUE),\n            S_PURPURATUS = sum(S_PURPURATUS, na.rm = TRUE),\n            M_FRANCISCANUS = sum(M_FRANCISCANUS, na.rm = TRUE),\n            TOTAL_URCHINS = sum(TOTAL_URCHINS, na.rm = TRUE),\n            Protection = mean(Protection))\n\n# Change class to TRUE/FALSE for use in ggplot shape aesthetic\nUrchins$Protection &lt;- as.logical(Urchins$Protection)\n\n\n\n\nTidy Data\nNext the entire dataframe is pivoted to be longer along each Species, so now each species per survey has its own row for count. For later analysis, I use this longer dataframe to simplify the counts for each species to 1 or 0 for presence or absence. This is saved as a new dataframe and widened to return to a survey per row.\n\n\nCode\n## Create Species Data Frame\nSpeciesDF &lt;- Urchins |&gt;\n  pivot_longer(cols = c(\"ARTHROPODA\", \"BIVALVIA\", \"GASTROPODA\", \"OPHIUROIDEA_ASTERPODEA\", \"S_PURPURATUS\", \"M_FRANCISCANUS\", \"TOTAL_URCHINS\"),\n               names_to = \"Species\",\n               values_to = \"Count\",\n               values_drop_na = TRUE)              # Drop NA values\n\n# Add a binary row for species presence \nSpeciesDF$Present[SpeciesDF$Count &gt;=1] &lt;- 1\nSpeciesDF$Present[SpeciesDF$Count ==0] &lt;- 0\n\n\n# Dataframe displays only binary presence for each species per survey\nPresentDF &lt;- SpeciesDF |&gt; \n  select(-Count) |&gt; \n  pivot_wider(values_from = \"Present\",\n              names_from = \"Species\")\n\n\n\n\n\n\nData Exploration\nThese initial visualizations can provide clues to trends in the data. This first plot shows the data distribution over time with total species count at each site, highlighting if protected or not. There are several major spikes, potential outliers, and many zeros. Overall this visualization is too crowded to infer any patterns.\n\n\nCode\nTotalUrchins &lt;- ggplot(Urchins, aes(x = DATE_RETRIEVED, y = TOTAL_URCHINS)) +\n  geom_point(aes(color = SITE, shape = Protection), alpha = .7, size = 2) +\n  theme_minimal() +\n  labs(title = \"Total Species Collected at Sites in Southern CA\",\n       y = \"Total Species Count\",\n       x = \"Date Collected\",\n       color = \"Site\", shape = \"Protection\") +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = .5)) +\n  scale_color_brewer(palette = \"Set1\")\nTotalUrchins\n\n\n\n\n\nBelow is a series of maps of species abundance over time for each survey site. There are high spikes in this data with a lot of zeros, and does not appear linear in nature for any of the sites. There is also not a single site, protection status or species that is the solo cause of the data spikes.\n\n\nCode\n## How does each species vary per site:\n\nAna &lt;- SpeciesDF |&gt; \n  filter(SITE == \"ANACAPA\") |&gt; \n  ggplot(aes(x = DATE_RETRIEVED, y = Count, color = Species)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Anacapa\",\n       x = NULL) +\n  ylab(\"Species Count\")\n\nFBPC &lt;- SpeciesDF |&gt; \n  filter(SITE == \"FBPC\") |&gt; \n  ggplot(aes(x = DATE_RETRIEVED, y = Count, color = Species)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Point Cabrillo\",\n       x = NULL,\n       y = NULL)\n\nGav &lt;- SpeciesDF |&gt; \n  filter(SITE == \"GAVIOTA\") |&gt; \n  ggplot(aes(x = DATE_RETRIEVED, y = Count, color = Species)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Gaviota\",\n       x = NULL) +\n  ylab(\"Species Count\")\n\nOB &lt;- SpeciesDF |&gt; \n  filter(SITE == \"OCNBCH\") |&gt; \n  ggplot(aes(x = DATE_RETRIEVED, y = Count, color = Species)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Ocean Beach\",\n       x = NULL,\n       y = NULL)\n\nEL &lt;- SpeciesDF |&gt; \n  filter(SITE == \"SBELL\") |&gt; \n  ggplot(aes(x = DATE_RETRIEVED, y = Count, color = Species)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Ellwood Pier\",\n       x = NULL) +\n  ylab(\"Species Count\")\n\nSW &lt;- SpeciesDF |&gt; \n  filter(SITE == \"SBSTWRF\") |&gt; \n  ggplot(aes(x = DATE_RETRIEVED, y = Count, color = Species)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Stearns Wharf\",\n       x = NULL,\n       y = NULL)\n\nScrip &lt;- SpeciesDF |&gt; \n  filter(SITE == \"SIO\") |&gt; \n  ggplot(aes(x = DATE_RETRIEVED, y = Count, color = Species)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Scripps\") +\n  xlab(\"Date Collected\") +\n  ylab(\"Species Count\")\n\nAV &lt;- SpeciesDF |&gt; \n  filter(SITE == \"AVILA\") |&gt; \n  ggplot(aes(x = DATE_RETRIEVED, y = Count, color = Species)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Avila Beach\",\n       y = NULL) +\n  xlab(\"Date Collected\")\n\nggarrange(Ana, FBPC, Gav, OB, EL, SW, Scrip, AV + rremove(\"x.text\"), \n          ncol = 2, nrow = 4,\n          common.legend = TRUE,\n          legend = \"right\")\n\n\n\n\n\n\n\n\nHypothesis Testing\nFirst, I want to check if there is a difference in means between protected and unprotected sites for each species with Welch’s Two Sample t-test. The goal is to understand if any species are sensitive to unprotected regions. The null hypothesis predicts there is no change between the abundance of each species in protected versus unprotected regions. I will repeat this for each listed species and total urchins.\n\\[H_0: \\mu_{protected} - \\mu_{unprotected} = 0 \\]\n\\[H_A: \\mu_{protected} - \\mu_{unprotected} \\neq  0 \\]\n\n\nCode\nProtect_Ttest &lt;- t.test(Urchins$TOTAL_URCHINS[Urchins$Protection == 1],\n       Urchins$TOTAL_URCHINS[Urchins$Protection == 0])\n# Significant\n\nArt_Ttest &lt;- t.test(Urchins$ARTHROPODA[Urchins$Protection == 1],\n       Urchins$ARTHROPODA[Urchins$Protection == 0])\n# Significant\n\nBiv_Ttest &lt;- t.test(Urchins$BIVALVIA[Urchins$Protection == 1],\n       Urchins$BIVALVIA[Urchins$Protection == 0])\n# Significant\n\nGast_Ttest &lt;- t.test(Urchins$GASTROPODA[Urchins$Protection == 1],\n       Urchins$GASTROPODA[Urchins$Protection == 0])\n# Significant\n\nOph_Ttest &lt;- t.test(Urchins$OPHIUROIDEA_ASTERPODEA[Urchins$Protection == 1],\n       Urchins$OPHIUROIDEA_ASTERPODEA[Urchins$Protection == 0])\n# Not Significant\n\nPurp_Ttest &lt;- t.test(Urchins$S_PURPURATUS[Urchins$Protection == 1],\n       Urchins$S_PURPURATUS[Urchins$Protection == 0])\n# Significant\n\nFran_Ttest &lt;- t.test(Urchins$M_FRANCISCANUS[Urchins$Protection == 1],\n       Urchins$M_FRANCISCANUS[Urchins$Protection == 0])\n# Not Significant\n\n\n\n\nCode\nkable(tribble(~Species, ~Confidence, ~Pvalue, ~Protected, ~Unprotected,\n      \"Total Urchins\", \"[3.29, 9.99]\", \"&lt;.001\", 12, 5,\n       \"Arthropoda\", \"[-12.97, -8.37]\", \"&lt;.001\", 6, 16,\n       \"Bivalvia\", \"[-121.12, -71.34]\", \"&lt;.001\", 44, 140,\n       \"Gastropoda\", \"[5.95, 15.7]\", \"&lt;.001\", 22, 11,\n       \"Ophiuroidea/Asterpodea\", \"[-.07, .53]\", \".13\", 1, 0,\n       \"S purpuratus\", \"[3.18, 9.82]\", \"&lt;.001\", 11, 5,\n       \"M franciscanus\", \"[-.05, .17]\", \".32\", 0, 0))\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nConfidence\nPvalue\nProtected\nUnprotected\n\n\n\n\nTotal Urchins\n[3.29, 9.99]\n&lt;.001\n12\n5\n\n\nArthropoda\n[-12.97, -8.37]\n&lt;.001\n6\n16\n\n\nBivalvia\n[-121.12, -71.34]\n&lt;.001\n44\n140\n\n\nGastropoda\n[5.95, 15.7]\n&lt;.001\n22\n11\n\n\nOphiuroidea/Asterpodea\n[-.07, .53]\n.13\n1\n0\n\n\nS purpuratus\n[3.18, 9.82]\n&lt;.001\n11\n5\n\n\nM franciscanus\n[-.05, .17]\n.32\n0\n0\n\n\n\n\n\nThis analysis found that some species are more sensitive to protection from coastal human development with low p-values, allowing the null hypothesis to be rejected for Total Urchins, Arthropoda, Bivalvia, Gastropoda, and S purpuratus. It is interesting to note that not all species were affected the same, the average means for Arthropoda and Bivalvia are higher in unprotected versus protected regions. This could be a result of changing ecological functions, a loss of predators, or another cause that human activities have been favorable for the species.\nNot all species displayed a sensitivity or difference between the regions. Ophiuroidea/Asterpodea and M franciscanus each had higher p-values (.13 and .32 respectively) so unable to reject the null hypothesis of no difference between the means of protected versus unprotected regions for these groups. It is notable that Total Urchins had a significant p-value, but that changes when the two urchin species are separated, as S purpuratus does not show a significant difference but M franciscanus does between the sites. This may need to be reviewed further for significance as M franciscanus appears to have generally low abundance in the data with an average count of 0 between both sites. This signifies an importance to measure the species separately instead of in the combined fashion in the dataset.\nThis plot displays for each species a confidence interval, for each there is a 95% probability that the true difference in means for the species count at protected versus unprotected sites falls within the confidence range.\n\n\n\nTime Series\n\\[\nSpeciesPresence = B_0 + B_1 date_t + B_2 protection + E_t\n\\]\nFinally, I want to explore if there is a time element with the presence of each species. To account for the large spikes, I utilize the dataframe with the binary present/absent data for each species per survey. The below time series linear regressions compare protected versus unprotected for each species.\n\n\nCode\nsummary(lm(TOTAL_URCHINS ~ DATE_RETRIEVED + Protection, data = PresentDF))\nTime_Urch &lt;- ggplot(PresentDF, aes(x = DATE_RETRIEVED, y = TOTAL_URCHINS, color = Protection)) +\n  geom_jitter(height = .1, alpha = .6) +\n  geom_smooth(method = loess, formula = 'y ~ x') +\n  theme_minimal() +\n  labs(title = \"Total Urchins\",\n       x = NULL,\n       y = NULL) +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = .5))\n\nsummary(lm(ARTHROPODA ~ DATE_RETRIEVED + Protection, data = PresentDF))\nTime_Art &lt;- ggplot(PresentDF, aes(x = DATE_RETRIEVED, y = ARTHROPODA, color = Protection)) +\n  geom_jitter(height = .1, alpha = .6) +\n  geom_smooth(method = loess, formula = 'y ~ x') +\n  theme_minimal() +\n  labs(title = \"Arthropoda\",\n       x = NULL,\n       y = NULL) +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = .5))\n\nsummary(lm(BIVALVIA ~ DATE_RETRIEVED + Protection, data = PresentDF))\nTime_Biv &lt;- ggplot(PresentDF, aes(x = DATE_RETRIEVED, y = BIVALVIA, color = Protection)) +\n  geom_jitter(height = .1, alpha = .6) +\n  geom_smooth(method = loess, formula = 'y ~ x') +\n  theme_minimal() +\n  labs(title = \"Bivalvia\",\n       x = NULL) +\n  ylab(\"Present (1) or Absent (0) At Survey\") +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = .5))\n\nsummary(lm(GASTROPODA ~ DATE_RETRIEVED + Protection, data = PresentDF))\nTime_Gast &lt;- ggplot(PresentDF, aes(x = DATE_RETRIEVED, y = GASTROPODA, color = Protection)) +\n  geom_jitter(height = .1, alpha = .6) +\n  geom_smooth(method = loess, formula = 'y ~ x') +\n  theme_minimal() +\n  labs(title = \"Gastropoda\",\n       x = NULL,\n       y = NULL) +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = .5))\n\nsummary(lm(OPHIUROIDEA_ASTERPODEA ~ DATE_RETRIEVED + Protection, data = PresentDF))\nTime_Oph &lt;- ggplot(PresentDF, aes(x = DATE_RETRIEVED, y = OPHIUROIDEA_ASTERPODEA, color = Protection)) +\n  geom_jitter(height = .1, alpha = .6) +\n  geom_smooth(method = loess, formula = 'y ~ x') +\n  theme_minimal() +\n  labs(title = \"Ophiuroida/Asterpodea\",\n       x = NULL,\n       y = NULL) +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = .5))\n\nsummary(lm(S_PURPURATUS ~ DATE_RETRIEVED + Protection, data = PresentDF))\nTime_Purp &lt;- ggplot(PresentDF, aes(x = DATE_RETRIEVED, y = S_PURPURATUS, color = Protection)) +\n  geom_jitter(height = .1, alpha = .6) +\n  geom_smooth(method = loess, formula = 'y ~ x') +\n  theme_minimal() +\n  labs(title = \"S Purpuratus\",\n       x = NULL,\n       y = NULL) +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = .5))\n\nsummary(lm(M_FRANCISCANUS ~ DATE_RETRIEVED + Protection, data = PresentDF))\nTime_Franc &lt;- ggplot(PresentDF, aes(x = DATE_RETRIEVED, y = M_FRANCISCANUS, color = Protection)) +\n  geom_jitter(height = .1, alpha = .6) +\n  geom_smooth(method = loess, formula = 'y ~ x') +\n  theme_minimal() +\n  labs(title = \"M franciscanus\",\n       y = NULL) +\n  xlab(\"Date Collected\") +\n  theme(plot.title = element_text(face = \"bold\",\n                                  hjust = .5))\n\n\n\n\nCode\nggarrange(Time_Urch, Time_Art, Time_Biv, Time_Gast, Time_Oph, Time_Purp, Time_Franc + rremove(\"x.text\"),\n          ncol = 2, nrow = 4,\n          common.legend = TRUE,\n          legend = \"right\")\n\n\n\n\n\nThere is a low p-value associated with time displaying significance in this variable for Arthropoda, Bivalvia, Gastropoda, and Ophiuroidea/Asterpodea. Time was not significant for Total Urchins, S purpuratus, or M franciscanus. Interesting to note, in this linear regression time series the protection was found to be significant for Ophiuroidea/Asterpodea and M franciscanus while the earlier t-test did not. Overall these regressions have low R squared values, which signify the proportion of the species presence explained by time and protection.\nOverall, I have found there are sensitivities to some species between protected and unprotected coastlines. Continued research could look further into the factors that affect these species, such as pollution levels due to high human activity or change of the ecosystem dynamics due to fishing and other recreational activities.\n\n\n\nReferences\nBritannica, The Editors of Encyclopaedia. “littoral zone”. Encyclopedia Britannica, 8 Aug. 2019, https://www.britannica.com/science/littoral-zone. Accessed 3 December 2022.\nSanta Barbara Coastal LTER, Steven C Schroeter, John Douglas Dixon, Thomas Ebert, and John Richards. 2022. SBC LTER: Settlement of urchins and other invertebrates, ongoing since 1990.LTER Network Member Node. https://pasta.lternet.edu/package/metadata/eml/knb-lter-sbc/52/11.\n\n\n\n\nCitationBibTeX citation:@online{dale2022,\n  author = {Dale, Erica},\n  title = {Statistics {Blog} {Post}},\n  date = {2022-12-09},\n  url = {http://ericamarie9016.github.io/2022-12-9-stats-proj},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nDale, Erica. 2022. “Statistics Blog Post.” December 9,\n2022. http://ericamarie9016.github.io/2022-12-9-stats-proj."
  },
  {
    "objectID": "posts/2022-12-02-geospatial/index.html",
    "href": "posts/2022-12-02-geospatial/index.html",
    "title": "Geospatial Blog Post",
    "section": "",
    "text": "This project was adapted from an assignment to explore the suitable habitats for species along the Western Coast, beginning with an Oyster and then creating a function to explore any species. The parameters to explore are depth and sea surface temperature. This could further be expanded with more variables, including change of temperature within the water column. These regions are broken down to five Exclusive Economic Zones (EEZ) along the West Coast of the US.\nThis could be applied for restoration efforts to determine where habitat is suitable for any oceanic species."
  },
  {
    "objectID": "posts/2022-12-02-geospatial/index.html#footnotes",
    "href": "posts/2022-12-02-geospatial/index.html#footnotes",
    "title": "Geospatial Blog Post",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGEBCO Compilation Group (2022) GEBCO_2022 Grid (doi:10.5285/e0f0bb80-ab44-2739-e053-6c86abc0289c).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erica Dale",
    "section": "",
    "text": "Hello world!\n\n\nWelcome to my world!\n\n\nBackground:\n\nMasters of Environmental Data Science:  Bren School of Environmental Science & Management - University of California, Santa Barbara, June 2023\nBachelors of Science in Biology, Concentration Organismal Biology and Ecology: Towson University, June 2016\nExperience with greenhouse, lab, and field research focused on botanical and ecological interactions. I am passionate to visualize ecosystem dynamics through computational modeling and apply data skills to conservation issues. I aim to work with nonprofits and government entities to utilize data science for science communication and decision making on global issues such as conservation, biodiversity loss, and environmental justice. With regular national and international travel, I enjoy expanding my perspective through hands-on lessons regarding other cultures and regional environmental dynamics.\n\nMy blog highlights essays and data focused projects from my Master’s program. The travel page overviews some of my background experiences."
  },
  {
    "objectID": "posts/2022-12-5-ethics-proj/index.html",
    "href": "posts/2022-12-5-ethics-proj/index.html",
    "title": "Ethics Blog Post",
    "section": "",
    "text": "My goal was to put together an argument about lack of data accessibility and rights to tribes for their own informed decision making. I introduce the issue with examples of industry taking advantage of the data and power gap that puts natives at risk (health and environmental) with resource extraction. I continue to argue how the field of conservation in Western hands has been at a loss for not including Indigenous traditional management and perspectives on ecological data available. I conclude with ongoing efforts to return land and power to these Indigenous groups.\n\nInternet and tool access:\nSmall communities and indigenous groups around the world have limited access to data collected on them and the tools for gathering and utilizing data. This data includes information about their citizens, communities, lands, resources, and culture which has been collected through “satellite research” by entities such as government and academia but not shared with the tribes. This leaves groups vulnerable due to lack of their own tools and infrastructure to incorporate data for decision making, and limited visibility of their concerns on a global scale. Internet data focused on the United States highlights that tribes have low availability of internet and few have affordable access, which is why just over half of Native American’s living on reservations with a computer have access to high-speed internet service. This is the lowest percent internet coverage in the nation, and a problem globally for Indigenous groups (Wang 2018). This invisible gap includes approximately 370 million Indigenous peoples living in 90 countries around the world (United 2009).\n\n\nEcological impact without Natives:\nThere is a larger ecological and conservation impact of these groups being isolated. Without internet tribes do not have educational access for the tools to analyze data, or access to the larger global community, which also means their voices are unheard in global conversations, including landscape conservation and resource management. As a result there is limited attention towards the industrial pressure on Indigenous lands. In addition, the knowledge and different perspectives within these tribes that could be brought into conservation efforts for landscape are overlooked. Keeping conservation sciences as “western” science isolates other perspectives and cultural significance of landscapes and ecosystems. Western science has dictated how we view data and how to use it, analyze it, and make decisions from it. Data can be used as a tool for Natives to tell stories and turn the information into art to understand the world in a different way, and weigh the data for their own informed decision making.\n\n\nIndustry takes advantage of the gap in data accessibility and land rights:\nThese regions have been taken advantage of by industry and the government, benefiting from this global blind spot on indigenous lands and environmental impacts. Indigenous land rights are only recognized in a few countries around the world, and even in these few there are complications with land titling and the governments have often leased out the lands for resource extraction without consultation. In many cases, this has led to indigenous groups forced to leave their lands due to the destruction of their self-sustaining ecosystems (Guri 2022). Resource extraction for mineral, oil and gas deposits has caused environmental degradation on native lands without the attention of the wider global arena interested in conservation efforts. Ultimately it is the native communities that have the negative fallout of environmental hazards, and they do not benefit from the extracted resources.\nThis is a global problem with western interests working with local governments to gain access to resources on Indigenous lands. Companies are based often in other countries, such as the Australian mining company Asumah Resources Limited entering the upper west region of Ghana for gold prospecting with support of the government. This included the Tanchara community, who noted a rapid decline in their sacred groves and other sites, including significant plant species becoming locally extinct and the loss of wildlife (Guri 2022). The natives are often unaware of the decisions, their rights, and the potential impact. In this case the threat to their sacred groves and natural sites was reviewed by an outside group (Center for Indigenous Knowledge and Organizational Development in Ghana); this center was able to speak up to the government on behalf of the Natives, who did not have the tools for the impact surveys and the communication medium (Guri 2022). This was a win for the community, but often there is no one looking when the rights of tribes are taken advantage of for industrial progress.\nGovernment policy intertwines with industry affecting these issues. Brazil’s indigenous lands are under threat under changing political agendas, with Bolsanaro’s campaign to opening up Indigenous lands to exploitation. He issued an executive order transferring regulation of Indigenous reserves from the government’s Indigenous Agency (FUNAI) to the Ministry of Agriculture, which is invested in agribusiness interests. Bolsanaro further announced plans to open Indigenous reserves to development without requiring Indigenous consent. Even with government checks returning some of the land rights to tribes, these actions encouraged a 40% increase of illegal miners entering Indigenous Territory, polluting rivers with mercury (Squires 2020).\n\n\n\nAerial shot of deforestation in the Karipuna Indigenous Territory in the state of Rondônia, in November 2021. Photo by Alexandre Cruz Noronha/Amazônia Real (CC BY-NC-ND 2.0).\n\n\n\n\nIndigenous tribes take care of their land when given rights and data access:\nIndigenous regions with proper protection are successful with their own interests of conservation, for example some of the world’s healthiest tropical forests are within protected Indigenous areas. The integrity of these forests, and overall biodiversity scores, are highest in Indigenous communities compared to any other land management with human development (Abulu 2022). A study reviewing 169 publications of different forms of conservation management found that conservation efforts without Indigenous groups resulted generally in ineffective conservation. Indigenous lands had higher levels of biodiversity and natural productivity than other managed lands (Dawson 2019). Supporting this finding, another study reviewing Indigenous-managed lands through Australia, Brazil, and Canada found these areas were richer in vertebrate species than existing government protected landscapes (Robbins 2021).\nTribes have a different perspective of conservation, connecting the landscape with art and culture and utilizing sustainable harvesting practices. There are different motivations for conserving the ecosystems, and knowledge of ecological interactions. An example of the different approaches to wildlife management can be found in the United State’s first tribal wilderness area, Mission Mountain Wilderness Area, which annually closes off 10,000 acres for grizzly bears to feed high in the mountains undisturbed (Robbins 2021). This extra consideration for the habits of the wildlife are unheard of in western management styles. Indigenous management styles have evolved over centuries with these cultures immersed in nature. The style of management differs due to the varied concepts of nature, Indigenous knowledge tending to be more holistic and presenting characteristics to wildlife that westerners only attribute to humans (Robbins 2021).\nTwo major issues are highlighted with Indigenous land rights, one is conservation of the ecosystems, and the other is conservation of the culture and people. The tribes culture and lifestyle is entwined with their surroundings and their management of the landscape, utilizing traditional knowledge to support wildlife and protect ancestral grounds. Traditional ecological knowledge had been lost in conservation, such as the traditional use of prescribed fires to manage resources (Robbins 2021). With great attention to the land, these groups knew how to use fire to manage wildlife habitat and forests, increasing ecological resilience, and encouraging growth of useful species (Robbins 2021). The tribes ensure a healthy landscape and wildlife populations that they can live with and utilize. This management style is more active, as Indigenous groups see the land to live with, versus the Western view of highly developing a region and keeping protected areas isolated.\n\n\nGoing Forward\n\n\n\nThe National Bison Range in Montana, now managed by the Confederated Salish and Kootenai Tribes. DAVE FITZPATRICK / U.S.FISH AND WILDLIFE SERVICE\n\n\nEquitable conservation provides local groups the tools and empowerment to be stewards of their land. Providing land tenure and management rights to Indigenous tribes to protect and conserve their lands will benefit conservation as a whole. With most of the high-integrity forests left occupied by Indigenous people, conservation efforts must consider protecting both the ecosystem and the people (Abulu 2022). Creation of protected park areas do not need to remove tribes to protect these ecosystems, it would be more beneficial to provide native’s around the world with land tenure rights and their own ability to manage the landscape. Ultimately Native voices should be uplifted to have a larger say in conservation efforts outside of their reservations within their historic territories.\nAcross the United States, there has been a slow movement in recent years to return land to Indigenous tribes. A previously seized 18000 acres of land set aside for bison conservation has been returned to the tribes. Bison were plentiful before colonization and development, so it is cruelly ironic to have taken the land and bison away from the Indigenous groups who were properly managing the populations before. The U.S. government attempt at managing these populations was isolated from the ancestral knowledge the tribes had of the animals, displaying the lack of data transfer between both parties. These animals are considered almost familial to the tribes who once relied on their abundance for food, clothing, and shelter. The change in management back to the tribes has changed the focus of conservation, acknowledging the animal’s characteristics that were otherwise overlooked in Western management practices. The native’s management of these herds recognize the importance of family groups and practice to keep these groups together (Robbins 2021).\nSome tribes are able to work within governmental processes to find access to their ancestral lands. The Australian government has bought up a series of farm properties with the goal to restore the beneficial wetlands that were degraded from agriculture and water diversion. Proposals were accepted for the management of these wetlands and ultimately gave the right to the Indigenous group Nari Nari who had inhabited the region for 50,000 years (Robbins 2021). This ancestral knowledge held within this tribe is unavailable to outside sources, and invaluable to restore and protect the landscape they lived so closely with. An increase of co-management with Natives is also benefiting conservation of wildlife and plants utilizing traditional knowledge and practices.\nOne larger attempt to bring data collection and the power of data to Indigenous groups is through the Indigenous Navigator, focusing on Indigenous rights including land and conservation rights. The Indigenous Navigator was created with respect of the diversity of values and cultures globally. The purpose is to assist with data collection to support each group’s innovation to “contemporary problem-solving through community assessments, mapping, environmental tracking, and support for their livelihoods and social enterprises” (Calatan 2022). Community-based monitoring and information systems allow data to be collected from within the indigenous groups, and are then connected with the Indigenous Navigator to relevant actors on a global scale. This organization collectively attacks the problem of data rights by providing the groups ability to collect their own data, and bringing visibility and connectivity regarding problems faced by the groups.\nThe global conservation movement would be stronger with the allies of Indigenous peoples, providing these groups with the resources and voice to speak up. There needs to be discussion on not just what to conserve, but how to conserve it and who manages it. To tackle these issues, Indigenous tribes should have access on data collected on the Native’s land from outside groups such as government, academic, and industry. Affordable internet and training opportunities for tribal members to learn how to use the tools needs to be available. Each tribe should be given the rights to decide how to collect and use their data. These steps will provide Indigenous groups the ability to make their own sovereign decisions, and unite ancestral perspectives with today’s technology to empower their voices in conservation on a global scale.\n\n\nReferences\nUnited Nations Department of Economic and Social Affairs. (2009). State of the world’s Indigenous peoples. New York, NY: United Nations Publications.  https://www.un.org/esa/socdev/unpfii/documents/SOWIP/en/SOWIP_web.pdf\nWang, Hansi Lo. “Native Americans On Tribal Land Are ‘The Least Connected’ To High-Speed Internet.” NPR, December 6, 2018. https://www.npr.org/2018/12/06/673364305/native-americans-on-tribal-land-are-the-least-connected-to-high-speed-internet.\nAbulu, Latoya, and Laurel Sutherland. “Indigenous Lands Hold the World’s Healthiest Forests - but Only When Their Rights Are Protected.” Mongabay, Indigenous Peoples and Conservation, November 7, 2022. https://news.mongabay.com/2022/11/indigenous-lands-hold-the-worlds-healthiest-forests-but-only-when-their-rights-are-protected/ \nRobbins, Jim. “How Returning Lands to Native Tribes Is Helping Protect Nature.” Yale Environment 360, June 3, 2021. https://e360.yale.edu/features/how-returning-lands-to-native-tribes-is-helping-protect-nature.\nCalatan, Jimrex. “Raising Indigenous Voices through the Indigenous Navigator Framework.” TebTebba, May 28, 2022. https://tebtebba.org/index.php/news-and-updates/raising-indigenous-voices-through-the-indigenous-navigator-framework-indigenous-data-to-secure-rights.\n\nGuri, Bernard. “Rescuing Tanchara Community Lands from Gold Mining Through Biocultural Community Protocols.” Cultural Survival, August 31, 2022. https://www.culturalsurvival.org/publications/cultural-survival-quarterly/rescuing-tanchara-community-lands-gold-mining-through.\nMaffi, L. (Ed.). (2001). On Biocultural Diversity: Linking Language, Knowledge, and the Environment. Washington and London: Smithsonian Institution Press.\nDawson, N. M., B. Coolsaet, E. J. Sterling, R. Loveridge, N. D. Gross-Camp, S. Wongbusarakum, K. K. Sangha, L. M. Scherl, H. Phuong Phan, N. Zafra-Calvo, W. G. Lavey, P. Byakagaba, C. J. Idrobo, A. Chenet, N. J. Bennett, S. Mansourian, and F. J. Rosado-May. 2021. The role of Indigenous peoples and local communities in effective and equitable conservation. Ecology and Society 26(3):19. https://doi.org/10.5751/ES-12625-260319\n\nSquires, Carter, Kelsey Landau, and Robin Lewis. “Uncommon Ground: The Impact of Natural Resource Corruption on Indigenous Peoples.” Brookings, August 7, 2020. https://www.brookings.edu/blog/up-front/2020/08/07/uncommon-ground-the-impact-of-natural-resource-corruption-on-indigenous-\n\n\n\n\nCitationBibTeX citation:@online{dale2022,\n  author = {Dale, Erica},\n  title = {Ethics {Blog} {Post}},\n  date = {2022-12-05},\n  url = {http://ericamarie9016.github.io/2022-12-5-ethics-proj},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nDale, Erica. 2022. “Ethics Blog Post.” December 5, 2022. http://ericamarie9016.github.io/2022-12-5-ethics-proj."
  },
  {
    "objectID": "posts/2023-02-22-spotify/index.html",
    "href": "posts/2023-02-22-spotify/index.html",
    "title": "Spotify With Machine Learning",
    "section": "",
    "text": "This fun project will use my personal Spotify music, along with my friend Kiran’s, with the goal to build several machine learning algorithms that will determine whose music library a song belongs to. I will explore several candidate models (k nearest neighbors, bagging, and random forest) to predict this binary outcome. To begin, the code to access your own Spotify account is included!\n\nlibrary(spotifyr)         # Spotify API interaction\nlibrary(here)             # Set file location\nlibrary(knitr)            # Creates nice tables\nlibrary(tidyverse)        # Data manipulation and visualization\nlibrary(tidymodels)       # Building machine learning models\nlibrary(rsample)          # Prepocessing datasets for machine learning\nlibrary(readr)            # Reads structured data files\nlibrary(dplyr)            # Data manipulation and transformation\nlibrary(ggplot2)          # Visualizations and plots\nlibrary(rpart)            # Decision tree algorithms\nlibrary(caret)            # Tools for machine learning models\nlibrary(rpart.plot)       # Visualization of decision trees\nlibrary(vip)              # Computes variable importance\nlibrary(pdp)              # Visualization of partial dependence plots\nlibrary(parsnip)          # Creating and tuning machine learning models\nlibrary(ipred)            # Bagging and bootstrapping for ensemble models\nlibrary(baguette)         # Building deep learning models\n\n\n\nTo access the Spotify API, follow the link to Spotify For Developers (https://developer.spotify.com/) and follow these instructions:\n\nSelect “Create a Client ID”\nFill out form to create an app\nOn dashboard page, click new app\nApp’s dashboard page will have Client ID\nClick “Show Client Secret”\nUse the below code with your client ID and Client Secret in R!\n\n\nSys.setenv(SPOTIFY_CLIENT_ID = 'your_token')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'your_token')\n \naccess_token &lt;- get_spotify_access_token(\n   client_id = Sys.getenv(\"SPOTIFY_CLIENT_ID\"),\n   client_secret = Sys.getenv(\"SPOTIFY_CLIENT_SECRET\")\n)\n\nI downloaded my liked songs, but the built-in function with the {spotifyr} package has a limit to download only 20 songs at a time. Below I wrote a loop to continue adding all of my liked songs into a dataframe.\n\nsongs_data &lt;- data.frame()     # create base empty data frame\noffset &lt;- 0                    # starting point for spotify function offset\nlimit &lt;- 20                    # maximum download at a time\n\nwhile(TRUE) {                  # loop through all liked songs\n\n  tracks &lt;- get_my_saved_tracks(limit = limit, offset = offset)\n  \n  if(length(tracks) == 0) {    # setting when to stop the loop\n    break\n  }\n  \n  # add tracks into previously created dataframe\n  songs_data &lt;- rbind(songs_data, tracks)  \n  \n  offset &lt;- offset + limit     # reset the loop to start at the next 20\n\n}\n\nThere are other functions to play with inside this {spotifyr} package! I will not be exploring these further in this blog post.\n\nbearicas_recent &lt;- get_my_recently_played()\nbearicas_top &lt;- get_my_top_artists_or_tracks()\nunique(bearicas_top$genres)\n\nThis initial data downloaded is not very exciting to play with. This data frame is mostly important to pull out the song ID column, and use that to connect back with Spotify’s API for downloading the specific audio features for each song. This function has a maximum download of 100 rows at a time so I created another loop below to download all the related audio features and bind the columns to the initial dataframe.\n\naudio_features &lt;- data.frame()        # create base empty data frame\n\nfor(i in seq(from = 1, to = nrow(songs_data), by = 100)) { \n  \n  if (i &gt; nrow(songs_data)) {         # setting when to stop the loop\n    break\n  }\n  \n  row_index &lt;- i:(i + 99)             # collect 100 rows starting from i\n  \n  # pull out features for set rows\n  audio &lt;- get_track_audio_features(songs_data$track.id[row_index])\n  \n  # add features to dataframe\n  audio_features &lt;- rbind(audio_features, audio)\n}\n\n# will read in by 100, so may have NA's from the last loop\naudio_features &lt;- drop_na(audio_features)\n\n# create data frame with songs and fun features!\nericas_audio &lt;- cbind(audio_features, \n                      track.name = songs_data$track.name,\n                      track.popularity = songs_data$track.popularity) |&gt; \n  select(-c(uri, track_href, analysis_url, type))     # remove rows\n  \n# save as csv to share\nwrite_csv(ericas_audio, \"ericas_audio.csv\")\n\nMy friend Kiran and I swapped data, which I will use to create a series of machine learning models to compare our music tastes. The goal is to create a model that can predict, using the audio features whose playlist it is from. I will go through four different types of models and at the end compare the metrics of each to decide which was most effective! The outcome variable will be the binary option of Kiran or Erica, set as listener_id.\n\nericas_audio &lt;- ericas_audio |&gt; \n  mutate(listener_id = \"erica\")\n\nkirans_audio &lt;- read_csv(\"kiran_audio.csv\") |&gt;  # get partner's data as csv\n  mutate(listener_id = \"kiran\")\n\n# combine datasets\ntotal_audio &lt;- rbind(ericas_audio, kirans_audio) |&gt; \n  mutate(listener_id = as.factor(listener_id))\nwrite_csv(total_audio, \"total_audio.csv\")\n\nAll of these previous steps culminate to this total_audio.csv file that I have previously saved and set aside, since I did not want to share my private Spotify information at the beginning.\n\ntotal_audio &lt;- read_csv(here(\"posts\", \"2023-02-22-spotify\", \"total_audio.csv\")) |&gt; \n    mutate(listener_id = as.factor(listener_id))\n\n\n\n\n\n\nCode\ntotal_audio %&gt;%\n  arrange(desc(instrumentalness)) |&gt; \n  select(instrumentalness, track.name, track.popularity, listener_id) |&gt; \n  rename('track name' = track.name,\n         'track popularity' = track.popularity,\n         'listener' = listener_id) |&gt; \n  head(6) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\ninstrumentalness\ntrack name\ntrack popularity\nlistener\n\n\n\n\n0.971\nSlow Blues - Instrumental\n38\nkiran\n\n\n0.946\nOrange\n42\nkiran\n\n\n0.935\nYlang Ylang\n61\nkiran\n\n\n0.924\nAtlas\n46\nkiran\n\n\n0.924\nDefect\n19\nkiran\n\n\n0.918\nMaster Tea\n0\nerica\n\n\n\n\n\nSurprised to find out that the top instrumental songs belonged mostly to Kiran’s playlist, I mostly listen to music with strong drums and little lyrics so expected that I’d be in the top.\n\n\nCode\ntotal_audio %&gt;%\n  arrange(desc(acousticness)) |&gt; \n  select(acousticness, track.name, track.popularity, listener_id) |&gt; \n  rename('track name' = track.name,\n         'track popularity' = track.popularity,\n         'listener' = listener_id) |&gt; \n  head(6) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nacousticness\ntrack name\ntrack popularity\nlistener\n\n\n\n\n0.994\nPachamama\n58\nerica\n\n\n0.979\nFlowers\n57\nerica\n\n\n0.978\nAll We Do\n56\nerica\n\n\n0.973\nWhatever’s Written in Your Heart\n29\nkiran\n\n\n0.942\nThe Forsaken Waltz\n32\nkiran\n\n\n0.934\nThe View\n8\nerica\n\n\n\n\n\nAlthough more of the top acoustic songs belonged in my playlist, Kiran listens to much louder music than me apparently.\n\n\nCode\nggplot(total_audio, aes(x = track.popularity, y = listener_id)) +\n  geom_boxplot(aes(fill = listener_id), color = \"#000000\", alpha = .8) +\n  labs(title = \"Distribution of Track Popularity by Listener\",\n       x = \"Track Popularity\", y = \"Listener\") +\n  scale_fill_manual(values = c(\"#9954FE\", \"#289832\")) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        panel.grid.minor = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(colour = \"black\"),\n        legend.position = \"none\")\n\n\n\n\n\nI listen to the most music listed as 0 popularity, so maybe I’m more underground and edgy with my style.\n\n\nCode\nggplot(total_audio, aes(x = danceability, y = energy)) +\n  geom_point(aes(color = listener_id), alpha = .8, size = 2) +\n  labs(title = \"Comparison of Dancing Styles\",\n       x = \"Danceability\", y = \"Energy\") +\n  scale_color_manual(values = c(\"#9954FE\", \"#289832\"),\n                     labels = c(\"Erica\", \"Kiran\")) +\n  geom_text(x = .99, y = .97, label = \"Party Dancing\",\n            color = \"black\", size = 4, hjust = 1, vjust = 1) +\n  geom_text(x = 0.95, y = 0.2, label = \"Slow Dancing\",\n            color = \"black\", size = 3.5, hjust = 1, vjust = 0) +\n  geom_text(x = 0.2, y = 0.2, label = \"Chill Zone\",\n            color = \"black\", size = 3.5, hjust = 0, vjust = 0) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        panel.grid.minor = element_blank(),\n        axis.line = element_line(colour = \"black\"),\n        legend.position = \"bottom\")\n\n\n\n\n\nWith this graph, low energy and high danceability would relate to slower (possibly romantic) songs, both of us appear to enjoy high energy and very danceable music but Kiran definitely goes harder.\n\n\n\nI will be creating several machine learning models, and use these variables as the start for them all. This initial train/test split is an important step to divide the dataset in two subsets. The training data will be used throughout to build each model. The test data will only be used once for each model at the end to evaluate the performance of the model on before unseen data. Keeping the data separated avoids leakage, which happens when the final testing data has influence the building of the model.\n\nset.seed(61234)    # allows reproducibility\n\nsong_split &lt;- initial_split(total_audio)\nsong_test &lt;- testing(song_split) \nsong_train &lt;- training(song_split)\n\n# Preprocessing, creating recipe with outcome and predictors\nsong_recipe &lt;- recipe(listener_id ~ ., data = song_train) |&gt; \n  \n  # Keep data but do not use are predictor\n  update_role(track.name, new_role = \"ID\") |&gt;   \n  update_role(id, new_role = \"ID\") |&gt; \n  step_rm(track.name, id) |&gt; \n  \n  # Dummy code and normalize predictors\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) |&gt; \n  step_normalize(all_numeric(), -all_outcomes()) |&gt; \n  \n  prep()\n\n# Cross Validation to tune parameter\ncv_folds &lt;- song_train |&gt; \n  vfold_cv(v = 5)\n\n\n\n\nThis is a type of supervised machine learning algorithm used for classification and regression tasks. In this case, I will be using it as classification because we have the binary output variable of listener_id. When predicting the value of an input data point, this model looks for the “K” closest data points within the training set. The output prediction is based on the majority class or mean value of the K neighbors.\n\nset.seed(45634)\n\n# Define nearest neighbor model\nknn_spec &lt;- nearest_neighbor(neighbors = 7) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\")\n\n# Workflow\nknn_workflow &lt;- workflow() |&gt; \n  add_model(knn_spec) |&gt; \n  add_recipe(song_recipe)\n\n# Fit resamples\nknn_res &lt;- knn_workflow |&gt; \n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE))\n\n# Check Performance\nknn_res |&gt; collect_metrics()\n\n# Tune the hyperparameters\nknn_spec_tune &lt;- nearest_neighbor(neighbors = tune()) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\")\n\n# Workflow: Define new workflow\nknn_workflow_tune &lt;- workflow() |&gt; \n  add_model(knn_spec_tune) |&gt; \n  add_recipe(song_recipe)\n\n# Fit workflow on predefined folds and hyperparameters\nknn_cv_fit &lt;- knn_workflow_tune |&gt; \n  tune_grid(\n    cv_folds,\n    grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10))))\n\n# Check performance\nknn_cv_fit |&gt; collect_metrics()\n# Results will show the n averaged over all the folds. Use this to predict the best.\n\n# Workflow: Final\nknn_final_wf &lt;- knn_workflow_tune |&gt; \n  finalize_workflow(select_best(knn_cv_fit, metric = \"accuracy\"))\n\n# Fit: Final\nknn_final_fit &lt;- knn_final_wf |&gt; fit(song_train)\nknn_last_fit &lt;- knn_final_wf |&gt; last_fit(song_split)\nknn_metrics &lt;- knn_last_fit |&gt; collect_metrics()\n\n# Predict labels for test set\nknn_pred &lt;- predict(knn_final_fit,\n                          new_data = song_test)\n\n# Pull out actual listener\nsong_test_true &lt;- song_test %&gt;%\n  select(listener_id)\n\n# Evaluate model performance on test set\nknn_perf &lt;- knn_pred %&gt;%\n  bind_cols(song_test_true)\n\n\n# View predicted and actual listeners\nknn_perf |&gt; \n  select(Predicted = .pred_class, Actual = listener_id) |&gt; \n  slice(1:10) |&gt; \n  kable()\n\n\n\n\nPredicted\nActual\n\n\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\n\n\nknn_perf |&gt; \n  metrics(truth = listener_id, estimate = .pred_class)\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.692\n2 kap      binary         0.378\n\n\n\n\n\nThe next two models, bagging and random forests, use a series of decision trees. Decision tree models begin with all the data in a root node and making a split based on the most significant feature. Each split results in nodes of data to split on another feature, until there are finally no features left to split the data on. The model makes predictions by following the path from the root node to a leaf node following the rules set by each node split.\n\n\n\nBagging is a form of bootstrap aggregation, this means that this is an ensemble model. The model constructs multiple versions of the same base model using random samples of the training data. All of these sub-models are aggregated into a final model. These steps improve the model performance by reducing variance and overfitting.\n\nset.seed(4657345)\n\n# Tune specs\ntree_spec_tune &lt;- bag_tree(\n  mode = \"classification\",\n  cost_complexity = tune(),\n  tree_depth = tune(),\n  min_n = tune()) |&gt; \n  set_engine(\"rpart\", times = 50)\n\n# Define tree grid\ntree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)\n\n# New workflow\nwf_tree_tune &lt;- workflow() |&gt; \n  add_recipe(song_recipe) |&gt; \n  add_model(tree_spec_tune)\n\n# Build each model in parallel\ndoParallel::registerDoParallel()\n\n# Fit model\ntree_rs &lt;- wf_tree_tune |&gt;\n  tune_grid(listener_id ~ .,\n    resamples = cv_folds,\n    grid = tree_grid,\n    metrics = metric_set(accuracy))\n\n# Final workflow\nfinal_bag &lt;- finalize_workflow(wf_tree_tune, select_best(tree_rs, \"accuracy\")) |&gt; \n  fit(data = song_train)\n\n# Predictions\nbag_pred &lt;- final_bag |&gt; \n  predict(new_data = song_test) |&gt; \n  bind_cols(song_test)\n\n# Save metrics\nbag_metrics &lt;- bag_pred |&gt; \n  metrics(truth = listener_id, estimate = .pred_class)\n\n\n# View predicted and actual listeners\nbag_pred |&gt; \n  select(Predicted = .pred_class, Actual = listener_id) |&gt; \n  slice(1:10) |&gt; \n  kable()\n\n\n\n\nPredicted\nActual\n\n\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\n\n\n# Evaluate performance\nbag_pred |&gt; \n  metrics(truth = listener_id, estimate = .pred_class)\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.730\n2 kap      binary         0.455\n\n\n\n\n\nRandom forest is another ensemble model, but this one cannot be done in parallel. This method creates multiple decision trees on random subsets of the data, and the key difference with this model is the random selection of features to include for each model. Not using all the features in each model then combining the decision trees improves the accuracy of the model.\n\n# Define validating set\nset.seed(1368)\nval_set &lt;- validation_split(song_train, \n                            strata = listener_id, \n                            prop = 0.70)\n\n# Create Random Forest specification\nrf_spec &lt;-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 1000) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# Define Random Forest workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(song_recipe) %&gt;%\n  add_model(rf_spec)\n\n# Build in parallel\ndoParallel::registerDoParallel()\nrf_res &lt;- \n  rf_workflow %&gt;% \n  tune_grid(val_set,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(accuracy))\n\n# Output model metrics\nrf_res %&gt;% collect_metrics()\n\n# Find the best accuracy metric\nrf_res %&gt;% \n  show_best(metric = \"accuracy\")\n\n# Plot results\nautoplot(rf_res) +\n  theme_minimal()\n# Select best Random Forest model\nbest_rf &lt;- select_best(rf_res, \"accuracy\")\n\n# Output predictions\nrf_res %&gt;% \n  collect_predictions()\n\n# Defining final model while working in parallel\ndoParallel::registerDoParallel()\nlast_rf_model &lt;- \n  rand_forest(mtry = 2, min_n = 3, trees = 1000) %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"classification\")\n\n# Update workflow\nlast_rf_workflow &lt;- \n  rf_workflow %&gt;% \n  update_model(last_rf_model)\n\n# Update model fit\nrf_final_fit &lt;- last_rf_workflow |&gt; fit(song_train)\n\nlast_rf_fit &lt;- \n  last_rf_workflow %&gt;% \n  last_fit(song_split)\n\n# Output model metrics\nrandom_forest_metrics &lt;- last_rf_fit %&gt;% \n  collect_metrics()\n\n# Predict on test set\nrf_pred &lt;- predict(rf_final_fit,\n                     new_data = song_test) |&gt; \n  bind_cols(song_test)\n\n\n# Output the variables that are most important to our model\nlast_rf_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip(num_features = 12) +\n  ggtitle(\"Variable Importance Plot\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme_minimal()\n\n\n\n\n\n# View predicted and actual listeners\nrf_pred |&gt; \n  select(Predicted = .pred_class, Actual = listener_id) |&gt; \n  slice(1:10) |&gt; \n  kable()\n\n\n\n\nPredicted\nActual\n\n\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\n\n\n# Evaluate performance\nrf_pred |&gt; \n  metrics(truth = listener_id, estimate = .pred_class)\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.730\n2 kap      binary         0.453\n\n\n\n\n\n\n\nCode\n# nearest neighbors metrics\nknn_accuracy &lt;- knn_metrics$.estimate[1]\n \n# bag tree metrics\nbag_accuracy &lt;- bag_metrics$.estimate[1]\n\n# Random Forest metrics\nrf_accuracy &lt;- random_forest_metrics$.estimate[1]\n\n\nmodel_accuracy &lt;- tribble(\n  ~\"model\", ~\"accuracy\",\n  \"KNN\", knn_accuracy,\n  \"Bagging\", bag_accuracy,\n  \"Random Forest\", rf_accuracy\n)\n\nggplot(data = model_accuracy, aes(x = model, y = accuracy)) +\n         geom_col(fill = \"gray\") +\n  theme_minimal() +\n  labs(title = \"Comparison of Model Accuracy for Spotify Data\",\n       x = \"Model\", y = \"Accuracy\") +\n  theme(plot.title = element_text(size = 16, face = \"bold\"),\n        axis.title = element_text(size = 12),\n        axis.text = element_text(size = 10)) +\n  ylim(0,1)"
  },
  {
    "objectID": "posts/2023-02-22-spotify/index.html#introduction",
    "href": "posts/2023-02-22-spotify/index.html#introduction",
    "title": "Spotify With Machine Learning",
    "section": "",
    "text": "This fun project will use my personal Spotify music, along with my friend Kiran’s, with the goal to build several machine learning algorithms that will determine whose music library a song belongs to. I will explore several candidate models (k nearest neighbors, bagging, and random forest) to predict this binary outcome. To begin, the code to access your own Spotify account is included!\n\nlibrary(spotifyr)         # Spotify API interaction\nlibrary(here)             # Set file location\nlibrary(knitr)            # Creates nice tables\nlibrary(tidyverse)        # Data manipulation and visualization\nlibrary(tidymodels)       # Building machine learning models\nlibrary(rsample)          # Prepocessing datasets for machine learning\nlibrary(readr)            # Reads structured data files\nlibrary(dplyr)            # Data manipulation and transformation\nlibrary(ggplot2)          # Visualizations and plots\nlibrary(rpart)            # Decision tree algorithms\nlibrary(caret)            # Tools for machine learning models\nlibrary(rpart.plot)       # Visualization of decision trees\nlibrary(vip)              # Computes variable importance\nlibrary(pdp)              # Visualization of partial dependence plots\nlibrary(parsnip)          # Creating and tuning machine learning models\nlibrary(ipred)            # Bagging and bootstrapping for ensemble models\nlibrary(baguette)         # Building deep learning models\n\n\n\nTo access the Spotify API, follow the link to Spotify For Developers (https://developer.spotify.com/) and follow these instructions:\n\nSelect “Create a Client ID”\nFill out form to create an app\nOn dashboard page, click new app\nApp’s dashboard page will have Client ID\nClick “Show Client Secret”\nUse the below code with your client ID and Client Secret in R!\n\n\nSys.setenv(SPOTIFY_CLIENT_ID = 'your_token')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'your_token')\n \naccess_token &lt;- get_spotify_access_token(\n   client_id = Sys.getenv(\"SPOTIFY_CLIENT_ID\"),\n   client_secret = Sys.getenv(\"SPOTIFY_CLIENT_SECRET\")\n)\n\nI downloaded my liked songs, but the built-in function with the {spotifyr} package has a limit to download only 20 songs at a time. Below I wrote a loop to continue adding all of my liked songs into a dataframe.\n\nsongs_data &lt;- data.frame()     # create base empty data frame\noffset &lt;- 0                    # starting point for spotify function offset\nlimit &lt;- 20                    # maximum download at a time\n\nwhile(TRUE) {                  # loop through all liked songs\n\n  tracks &lt;- get_my_saved_tracks(limit = limit, offset = offset)\n  \n  if(length(tracks) == 0) {    # setting when to stop the loop\n    break\n  }\n  \n  # add tracks into previously created dataframe\n  songs_data &lt;- rbind(songs_data, tracks)  \n  \n  offset &lt;- offset + limit     # reset the loop to start at the next 20\n\n}\n\nThere are other functions to play with inside this {spotifyr} package! I will not be exploring these further in this blog post.\n\nbearicas_recent &lt;- get_my_recently_played()\nbearicas_top &lt;- get_my_top_artists_or_tracks()\nunique(bearicas_top$genres)\n\nThis initial data downloaded is not very exciting to play with. This data frame is mostly important to pull out the song ID column, and use that to connect back with Spotify’s API for downloading the specific audio features for each song. This function has a maximum download of 100 rows at a time so I created another loop below to download all the related audio features and bind the columns to the initial dataframe.\n\naudio_features &lt;- data.frame()        # create base empty data frame\n\nfor(i in seq(from = 1, to = nrow(songs_data), by = 100)) { \n  \n  if (i &gt; nrow(songs_data)) {         # setting when to stop the loop\n    break\n  }\n  \n  row_index &lt;- i:(i + 99)             # collect 100 rows starting from i\n  \n  # pull out features for set rows\n  audio &lt;- get_track_audio_features(songs_data$track.id[row_index])\n  \n  # add features to dataframe\n  audio_features &lt;- rbind(audio_features, audio)\n}\n\n# will read in by 100, so may have NA's from the last loop\naudio_features &lt;- drop_na(audio_features)\n\n# create data frame with songs and fun features!\nericas_audio &lt;- cbind(audio_features, \n                      track.name = songs_data$track.name,\n                      track.popularity = songs_data$track.popularity) |&gt; \n  select(-c(uri, track_href, analysis_url, type))     # remove rows\n  \n# save as csv to share\nwrite_csv(ericas_audio, \"ericas_audio.csv\")\n\nMy friend Kiran and I swapped data, which I will use to create a series of machine learning models to compare our music tastes. The goal is to create a model that can predict, using the audio features whose playlist it is from. I will go through four different types of models and at the end compare the metrics of each to decide which was most effective! The outcome variable will be the binary option of Kiran or Erica, set as listener_id.\n\nericas_audio &lt;- ericas_audio |&gt; \n  mutate(listener_id = \"erica\")\n\nkirans_audio &lt;- read_csv(\"kiran_audio.csv\") |&gt;  # get partner's data as csv\n  mutate(listener_id = \"kiran\")\n\n# combine datasets\ntotal_audio &lt;- rbind(ericas_audio, kirans_audio) |&gt; \n  mutate(listener_id = as.factor(listener_id))\nwrite_csv(total_audio, \"total_audio.csv\")\n\nAll of these previous steps culminate to this total_audio.csv file that I have previously saved and set aside, since I did not want to share my private Spotify information at the beginning.\n\ntotal_audio &lt;- read_csv(here(\"posts\", \"2023-02-22-spotify\", \"total_audio.csv\")) |&gt; \n    mutate(listener_id = as.factor(listener_id))\n\n\n\n\n\n\nCode\ntotal_audio %&gt;%\n  arrange(desc(instrumentalness)) |&gt; \n  select(instrumentalness, track.name, track.popularity, listener_id) |&gt; \n  rename('track name' = track.name,\n         'track popularity' = track.popularity,\n         'listener' = listener_id) |&gt; \n  head(6) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\ninstrumentalness\ntrack name\ntrack popularity\nlistener\n\n\n\n\n0.971\nSlow Blues - Instrumental\n38\nkiran\n\n\n0.946\nOrange\n42\nkiran\n\n\n0.935\nYlang Ylang\n61\nkiran\n\n\n0.924\nAtlas\n46\nkiran\n\n\n0.924\nDefect\n19\nkiran\n\n\n0.918\nMaster Tea\n0\nerica\n\n\n\n\n\nSurprised to find out that the top instrumental songs belonged mostly to Kiran’s playlist, I mostly listen to music with strong drums and little lyrics so expected that I’d be in the top.\n\n\nCode\ntotal_audio %&gt;%\n  arrange(desc(acousticness)) |&gt; \n  select(acousticness, track.name, track.popularity, listener_id) |&gt; \n  rename('track name' = track.name,\n         'track popularity' = track.popularity,\n         'listener' = listener_id) |&gt; \n  head(6) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nacousticness\ntrack name\ntrack popularity\nlistener\n\n\n\n\n0.994\nPachamama\n58\nerica\n\n\n0.979\nFlowers\n57\nerica\n\n\n0.978\nAll We Do\n56\nerica\n\n\n0.973\nWhatever’s Written in Your Heart\n29\nkiran\n\n\n0.942\nThe Forsaken Waltz\n32\nkiran\n\n\n0.934\nThe View\n8\nerica\n\n\n\n\n\nAlthough more of the top acoustic songs belonged in my playlist, Kiran listens to much louder music than me apparently.\n\n\nCode\nggplot(total_audio, aes(x = track.popularity, y = listener_id)) +\n  geom_boxplot(aes(fill = listener_id), color = \"#000000\", alpha = .8) +\n  labs(title = \"Distribution of Track Popularity by Listener\",\n       x = \"Track Popularity\", y = \"Listener\") +\n  scale_fill_manual(values = c(\"#9954FE\", \"#289832\")) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        panel.grid.minor = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(colour = \"black\"),\n        legend.position = \"none\")\n\n\n\n\n\nI listen to the most music listed as 0 popularity, so maybe I’m more underground and edgy with my style.\n\n\nCode\nggplot(total_audio, aes(x = danceability, y = energy)) +\n  geom_point(aes(color = listener_id), alpha = .8, size = 2) +\n  labs(title = \"Comparison of Dancing Styles\",\n       x = \"Danceability\", y = \"Energy\") +\n  scale_color_manual(values = c(\"#9954FE\", \"#289832\"),\n                     labels = c(\"Erica\", \"Kiran\")) +\n  geom_text(x = .99, y = .97, label = \"Party Dancing\",\n            color = \"black\", size = 4, hjust = 1, vjust = 1) +\n  geom_text(x = 0.95, y = 0.2, label = \"Slow Dancing\",\n            color = \"black\", size = 3.5, hjust = 1, vjust = 0) +\n  geom_text(x = 0.2, y = 0.2, label = \"Chill Zone\",\n            color = \"black\", size = 3.5, hjust = 0, vjust = 0) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        panel.grid.minor = element_blank(),\n        axis.line = element_line(colour = \"black\"),\n        legend.position = \"bottom\")\n\n\n\n\n\nWith this graph, low energy and high danceability would relate to slower (possibly romantic) songs, both of us appear to enjoy high energy and very danceable music but Kiran definitely goes harder.\n\n\n\nI will be creating several machine learning models, and use these variables as the start for them all. This initial train/test split is an important step to divide the dataset in two subsets. The training data will be used throughout to build each model. The test data will only be used once for each model at the end to evaluate the performance of the model on before unseen data. Keeping the data separated avoids leakage, which happens when the final testing data has influence the building of the model.\n\nset.seed(61234)    # allows reproducibility\n\nsong_split &lt;- initial_split(total_audio)\nsong_test &lt;- testing(song_split) \nsong_train &lt;- training(song_split)\n\n# Preprocessing, creating recipe with outcome and predictors\nsong_recipe &lt;- recipe(listener_id ~ ., data = song_train) |&gt; \n  \n  # Keep data but do not use are predictor\n  update_role(track.name, new_role = \"ID\") |&gt;   \n  update_role(id, new_role = \"ID\") |&gt; \n  step_rm(track.name, id) |&gt; \n  \n  # Dummy code and normalize predictors\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) |&gt; \n  step_normalize(all_numeric(), -all_outcomes()) |&gt; \n  \n  prep()\n\n# Cross Validation to tune parameter\ncv_folds &lt;- song_train |&gt; \n  vfold_cv(v = 5)\n\n\n\n\nThis is a type of supervised machine learning algorithm used for classification and regression tasks. In this case, I will be using it as classification because we have the binary output variable of listener_id. When predicting the value of an input data point, this model looks for the “K” closest data points within the training set. The output prediction is based on the majority class or mean value of the K neighbors.\n\nset.seed(45634)\n\n# Define nearest neighbor model\nknn_spec &lt;- nearest_neighbor(neighbors = 7) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\")\n\n# Workflow\nknn_workflow &lt;- workflow() |&gt; \n  add_model(knn_spec) |&gt; \n  add_recipe(song_recipe)\n\n# Fit resamples\nknn_res &lt;- knn_workflow |&gt; \n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE))\n\n# Check Performance\nknn_res |&gt; collect_metrics()\n\n# Tune the hyperparameters\nknn_spec_tune &lt;- nearest_neighbor(neighbors = tune()) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\")\n\n# Workflow: Define new workflow\nknn_workflow_tune &lt;- workflow() |&gt; \n  add_model(knn_spec_tune) |&gt; \n  add_recipe(song_recipe)\n\n# Fit workflow on predefined folds and hyperparameters\nknn_cv_fit &lt;- knn_workflow_tune |&gt; \n  tune_grid(\n    cv_folds,\n    grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10))))\n\n# Check performance\nknn_cv_fit |&gt; collect_metrics()\n# Results will show the n averaged over all the folds. Use this to predict the best.\n\n# Workflow: Final\nknn_final_wf &lt;- knn_workflow_tune |&gt; \n  finalize_workflow(select_best(knn_cv_fit, metric = \"accuracy\"))\n\n# Fit: Final\nknn_final_fit &lt;- knn_final_wf |&gt; fit(song_train)\nknn_last_fit &lt;- knn_final_wf |&gt; last_fit(song_split)\nknn_metrics &lt;- knn_last_fit |&gt; collect_metrics()\n\n# Predict labels for test set\nknn_pred &lt;- predict(knn_final_fit,\n                          new_data = song_test)\n\n# Pull out actual listener\nsong_test_true &lt;- song_test %&gt;%\n  select(listener_id)\n\n# Evaluate model performance on test set\nknn_perf &lt;- knn_pred %&gt;%\n  bind_cols(song_test_true)\n\n\n# View predicted and actual listeners\nknn_perf |&gt; \n  select(Predicted = .pred_class, Actual = listener_id) |&gt; \n  slice(1:10) |&gt; \n  kable()\n\n\n\n\nPredicted\nActual\n\n\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\n\n\nknn_perf |&gt; \n  metrics(truth = listener_id, estimate = .pred_class)\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.692\n2 kap      binary         0.378\n\n\n\n\n\nThe next two models, bagging and random forests, use a series of decision trees. Decision tree models begin with all the data in a root node and making a split based on the most significant feature. Each split results in nodes of data to split on another feature, until there are finally no features left to split the data on. The model makes predictions by following the path from the root node to a leaf node following the rules set by each node split.\n\n\n\nBagging is a form of bootstrap aggregation, this means that this is an ensemble model. The model constructs multiple versions of the same base model using random samples of the training data. All of these sub-models are aggregated into a final model. These steps improve the model performance by reducing variance and overfitting.\n\nset.seed(4657345)\n\n# Tune specs\ntree_spec_tune &lt;- bag_tree(\n  mode = \"classification\",\n  cost_complexity = tune(),\n  tree_depth = tune(),\n  min_n = tune()) |&gt; \n  set_engine(\"rpart\", times = 50)\n\n# Define tree grid\ntree_grid &lt;- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)\n\n# New workflow\nwf_tree_tune &lt;- workflow() |&gt; \n  add_recipe(song_recipe) |&gt; \n  add_model(tree_spec_tune)\n\n# Build each model in parallel\ndoParallel::registerDoParallel()\n\n# Fit model\ntree_rs &lt;- wf_tree_tune |&gt;\n  tune_grid(listener_id ~ .,\n    resamples = cv_folds,\n    grid = tree_grid,\n    metrics = metric_set(accuracy))\n\n# Final workflow\nfinal_bag &lt;- finalize_workflow(wf_tree_tune, select_best(tree_rs, \"accuracy\")) |&gt; \n  fit(data = song_train)\n\n# Predictions\nbag_pred &lt;- final_bag |&gt; \n  predict(new_data = song_test) |&gt; \n  bind_cols(song_test)\n\n# Save metrics\nbag_metrics &lt;- bag_pred |&gt; \n  metrics(truth = listener_id, estimate = .pred_class)\n\n\n# View predicted and actual listeners\nbag_pred |&gt; \n  select(Predicted = .pred_class, Actual = listener_id) |&gt; \n  slice(1:10) |&gt; \n  kable()\n\n\n\n\nPredicted\nActual\n\n\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\n\n\n# Evaluate performance\nbag_pred |&gt; \n  metrics(truth = listener_id, estimate = .pred_class)\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.730\n2 kap      binary         0.455\n\n\n\n\n\nRandom forest is another ensemble model, but this one cannot be done in parallel. This method creates multiple decision trees on random subsets of the data, and the key difference with this model is the random selection of features to include for each model. Not using all the features in each model then combining the decision trees improves the accuracy of the model.\n\n# Define validating set\nset.seed(1368)\nval_set &lt;- validation_split(song_train, \n                            strata = listener_id, \n                            prop = 0.70)\n\n# Create Random Forest specification\nrf_spec &lt;-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 1000) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# Define Random Forest workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(song_recipe) %&gt;%\n  add_model(rf_spec)\n\n# Build in parallel\ndoParallel::registerDoParallel()\nrf_res &lt;- \n  rf_workflow %&gt;% \n  tune_grid(val_set,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(accuracy))\n\n# Output model metrics\nrf_res %&gt;% collect_metrics()\n\n# Find the best accuracy metric\nrf_res %&gt;% \n  show_best(metric = \"accuracy\")\n\n# Plot results\nautoplot(rf_res) +\n  theme_minimal()\n# Select best Random Forest model\nbest_rf &lt;- select_best(rf_res, \"accuracy\")\n\n# Output predictions\nrf_res %&gt;% \n  collect_predictions()\n\n# Defining final model while working in parallel\ndoParallel::registerDoParallel()\nlast_rf_model &lt;- \n  rand_forest(mtry = 2, min_n = 3, trees = 1000) %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"classification\")\n\n# Update workflow\nlast_rf_workflow &lt;- \n  rf_workflow %&gt;% \n  update_model(last_rf_model)\n\n# Update model fit\nrf_final_fit &lt;- last_rf_workflow |&gt; fit(song_train)\n\nlast_rf_fit &lt;- \n  last_rf_workflow %&gt;% \n  last_fit(song_split)\n\n# Output model metrics\nrandom_forest_metrics &lt;- last_rf_fit %&gt;% \n  collect_metrics()\n\n# Predict on test set\nrf_pred &lt;- predict(rf_final_fit,\n                     new_data = song_test) |&gt; \n  bind_cols(song_test)\n\n\n# Output the variables that are most important to our model\nlast_rf_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip(num_features = 12) +\n  ggtitle(\"Variable Importance Plot\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme_minimal()\n\n\n\n\n\n# View predicted and actual listeners\nrf_pred |&gt; \n  select(Predicted = .pred_class, Actual = listener_id) |&gt; \n  slice(1:10) |&gt; \n  kable()\n\n\n\n\nPredicted\nActual\n\n\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\nkiran\nerica\n\n\nerica\nerica\n\n\nerica\nerica\n\n\n\n\n# Evaluate performance\nrf_pred |&gt; \n  metrics(truth = listener_id, estimate = .pred_class)\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.730\n2 kap      binary         0.453\n\n\n\n\n\n\n\nCode\n# nearest neighbors metrics\nknn_accuracy &lt;- knn_metrics$.estimate[1]\n \n# bag tree metrics\nbag_accuracy &lt;- bag_metrics$.estimate[1]\n\n# Random Forest metrics\nrf_accuracy &lt;- random_forest_metrics$.estimate[1]\n\n\nmodel_accuracy &lt;- tribble(\n  ~\"model\", ~\"accuracy\",\n  \"KNN\", knn_accuracy,\n  \"Bagging\", bag_accuracy,\n  \"Random Forest\", rf_accuracy\n)\n\nggplot(data = model_accuracy, aes(x = model, y = accuracy)) +\n         geom_col(fill = \"gray\") +\n  theme_minimal() +\n  labs(title = \"Comparison of Model Accuracy for Spotify Data\",\n       x = \"Model\", y = \"Accuracy\") +\n  theme(plot.title = element_text(size = 16, face = \"bold\"),\n        axis.title = element_text(size = 12),\n        axis.text = element_text(size = 10)) +\n  ylim(0,1)"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Posts",
    "section": "",
    "text": "Statistics Blog Post\n\n\n\n\n\n\n\nMEDS\n\n\nStatistics\n\n\nR\n\n\nCalifornia\n\n\nCoastal Ecology\n\n\nHypothesis Testing\n\n\nTime Series\n\n\n\n\nComparing Species Abundance off Protected and Developed Coastlines in Southern California\n\n\n\n\n\n\nDec 9, 2022\n\n\nErica Dale\n\n\n\n\n\n\n  \n\n\n\n\nSpotify With Machine Learning\n\n\n\n\n\n\n\nMEDS\n\n\nMachine Learning\n\n\nR\n\n\nSpotify\n\n\nMusic\n\n\nTutorial\n\n\n\n\nExploring Spotify Music with Different Machine Learning Algorithms\n\n\n\n\n\n\nDec 9, 2022\n\n\nErica Dale\n\n\n\n\n\n\n  \n\n\n\n\nEthics Blog Post\n\n\n\n\n\n\n\nMEDS\n\n\nEthics\n\n\nData Access\n\n\nConservation\n\n\nIndigenous Rights\n\n\n\n\nWhat Data Access Could Mean to Indigenous Groups and the Conservation Movement\n\n\n\n\n\n\nDec 5, 2022\n\n\nErica Dale\n\n\n\n\n\n\n  \n\n\n\n\nGeospatial Blog Post\n\n\n\n\n\n\n\nMEDS\n\n\nGeospatial\n\n\nR\n\n\nCoastal Ecology\n\n\n\n\nCreating a Function for Coastal Species Habitat\n\n\n\n\n\n\nDec 2, 2022\n\n\nErica Dale\n\n\n\n\n\n\nNo matching items"
  }
]