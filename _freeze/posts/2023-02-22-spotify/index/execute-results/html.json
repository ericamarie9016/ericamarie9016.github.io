{
  "hash": "a528fdf4f8fab92b37a76346c4198fe1",
  "result": {
    "markdown": "---\ntitle: \"Spotify With Machine Learning\"\ndescription: \"Exploring Spotify Music with Different Machine Learning Algorithms\"\nauthor:\n  - name: Erica Dale\n    url: http://ericamarie9016.githubt.io\n    affiliation: MEDS\n    affiliation-url: http://ucsb-meds.github.io\ndate: 2022-12-09\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\ncode-overflow: wrap\ncode-block-bg: true\ncode-block-border-left: \"#6B5A75\"\ncategories: [MEDS, Machine Learning, R, Spotify, Music]\ncitation: \n  url: http://ericamarie9016.github.io/2023-02-22-spotify\n# image:\ndraft: TRUE\n---\n\n\nNecessary libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spotifyr) #API interaction\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0     v purrr   1.0.1\nv tibble  3.1.8     v dplyr   1.1.0\nv tidyr   1.3.0     v stringr 1.5.0\nv readr   2.1.3     v forcats 1.0.0\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching packages -------------------------------------- tidymodels 1.0.0 --\nv broom        1.0.1     v rsample      1.1.0\nv dials        1.1.0     v tune         1.0.1\nv infer        1.0.3     v workflows    1.1.2\nv modeldata    1.0.1     v workflowsets 1.0.0\nv parsnip      1.0.3     v yardstick    1.1.0\nv recipes      1.0.3     \n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n* Search for functions across packages at https://www.tidymodels.org/find/\n```\n:::\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rpart)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n```\n:::\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n:::\n\n```{.r .cell-code}\nlibrary(rpart.plot)\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n```\n:::\n\n```{.r .cell-code}\nlibrary(pdp)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'pdp'\n\nThe following object is masked from 'package:purrr':\n\n    partial\n```\n:::\n\n```{.r .cell-code}\nlibrary(parsnip)\nlibrary(ipred)\nlibrary(baguette)\n```\n:::\n\n\n### My Notes to Clarify:::\nSpotify blog:\nThe Why: Utlimate goal is the determine binary Kiran or Erica. Don't necessarily know which one is the best, so explore multiple candidtae models for test data.\n\nAvoid leakage\nTrain and test separted, train data not used at all to influence test\nSplit data before preprocessing - indirect form of leakage\n- Preprocessing is {tidymodels} functions like step\n- Create recipe, part of preprocessing\n\nTHEN each model separately\n1. fit initial model parameters\n2. optimize hyperparameters, don't know ahead of time, specification of which hyperparameters to tune\n- tune with cross validation folds\n- tries a bunch of combos of hyperparameters with all the folds to give more attemps\n- makes a series of models, measure performance on fold (9 used, 1 test)\n3. fit final model\n\nKeeping track information\nProblem: Categorical variable and only one of each - useless variable and issue with NA's\n- dummy variable short circuits using this variable\nFix:\n1. API, each track has track id so keep that unique identifier\n2. Hold out variable from modelling process\n3. tidymodels specify that variable is neither predictor nor outcome??\n4. Ask Mateo further\n\nWhy do this\nFirst step in building recommender system - is person A or B more likely to like this song\n\nAnalysis:\nTypes of error, confusion matrix (false positives/negatives)\nDistribution of the predictions\n- Output is only 0/1 but internal is probability\n\n\n### First step, we need to access the Spotify API to download your data.\n\n*Client ID and Client Secret are required to create and access token that is required to interact with the API. You can set them as system values so we don't have to do provide them each time. Once you have an account, go to Spotify for developers (\\<https://developer.spotify.com/\\>) and log in. Click the green \"Create a Client ID\" button to fill out the form to create an app create an app so you can access the API. On your developer dashboard page, click on the new app you just created. On the app's dashboard page you will find your Client ID just under the header name of your app. Click \"Show Client Secret\" to access your secondary Client ID. When you do this you'll be issued a Spotify client ID and client secret key.*\n\nUse the below code with your own token and secret code to access your spotify data!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(SPOTIFY_CLIENT_ID = 'your_token')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'your_token')\n \naccess_token <- get_spotify_access_token(\n   client_id = Sys.getenv(\"SPOTIFY_CLIENT_ID\"),\n   client_secret = Sys.getenv(\"SPOTIFY_CLIENT_SECRET\")\n)\n```\n:::\n\n\nI downloaded my favorited songs, but the built-in function with the spotify package has a limit to download only 20 songs at a time. Below I created a loop to continue adding all of my liked songs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsongs_data <- data.frame()     # create base empty data frame\noffset <- 0                    # starting point for spotify function offset\nlimit <- 20                    # maximum download at a time\n\n# Loop through all liked songs\nwhile(TRUE) {\n  # the 20 downloaded tracks will temporarily save as a list called tracks\n  tracks <- get_my_saved_tracks(limit = limit, offset = offset)\n  \n  # setting when to stop the loop\n  if(length(tracks) == 0) {      \n    break\n  }\n  \n  # add tracks into previously created dataframe songs_data\n  songs_data <- rbind(songs_data, tracks)  \n  \n  # reset the loop to start at the next 20\n  offset <- offset + limit   \n}\n```\n:::\n\n\nThere are other functions to play with inside this {spotifyr} package! I will not be exploring these further in this blog post.\n\n::: {.cell}\n\n```{.r .cell-code}\nbears_recent <- get_my_recently_played()\nbears_top <- get_my_top_artists_or_tracks()\nunique(bears_top$genres)\n```\n:::\n\n\n\nAdd Audio Features: Looking at this initial playlist, there is a function within the spotify package to add audio features with the song id. This function has a maximum length of 100 so I created another loop below to download all the related audio features and bind the columns to the initial dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naudio_features <- data.frame()   # create base empty data frame\n\nfor(i in seq(from = 1, to = 283, by = 100)) { # loop through all songs\n  \n  # collect 100 rows starting from i\n  row_index <- i:(i + 99)   \n  \n  # pull out features for set rows\n  audio <- get_track_audio_features(songs_data$track.id[row_index])\n  \n  # add features to dataframe\n  audio_features <- rbind(audio_features, audio)\n}\n\n# Problem, is not stopping at 281 so is pulling out extra NA's\naudio_features <- drop_na(audio_features)\n\n# add songs_data$track.name\nericas_audio <- cbind(audio_features, \n                      track.name = songs_data$track.name,\n                      track.popularity = songs_data$track.popularity)\n\nericas_audio <- ericas_audio |> \n  select(-c(uri, track_href, analysis_url, type, id))\n  \n# save as csv to share\nwrite_csv(ericas_audio, \"ericas_audio.csv\")\n```\n:::\n\n\nMy classmate shared her prepared data with me, which I will use to create a series of machine learning models to compare our music tastes. I want to create a model that can predict, using the audio features who is the listener.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add column to each dataset (same name, different listener #)\nericas_audio <- ericas_audio |> \n  mutate(listener_id = \"erica\")\n\n# Get partner's data as csv\nkirans_audio <- read_csv(\"kiran_audio.csv\") |> \n  mutate(listener_id = \"kiran\") |> \n  select(-c(uri, track_href, analysis_url, type, id))\n\n# rbind datasets\ntotal_audio <- rbind(ericas_audio, kirans_audio) |> \n  mutate(listener_id = as.factor(listener_id))\nwrite_csv(total_audio, \"total_audio.csv\")\n```\n:::\n\n\nAll of these previous steps culminate to this total_audio.csv file that I have previously saved and set aside, since I did not want to share my private spotify information at the beginning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotal_audio <- read_csv(\"total_audio.csv\") |> \n    mutate(listener_id = as.factor(listener_id))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 634 Columns: 16\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (2): track.name, listener_id\ndbl (14): danceability, energy, key, loudness, mode, speechiness, acousticne...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n\n### Going to start with some fun data exploration!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Who listens to the most instrumental music\narrange(total_audio, desc(instrumentalness)) |> \n  head()\n# SURPRISED that Kiran does! I mostly listen to music with strong drums and little lyrics.\n\narrange(total_audio, desc(acousticness)) |> \n  head()\n# Not surprised that I listen to more acoustic, she listens to louder music\n\n# Comparing Track Popularity, make this a percent instead??\nggplot(total_audio, aes(x = track.popularity)) +\n  geom_bar(aes(fill = listener_id), alpha = .5) +\n  labs(title = \"Who Has More Popular Music Taste\")\n# Erica listens to the most 0 popularity music, meaning she's more underground and edgy. Kiran is basic.\n\nggplot(total_audio, aes(x = danceability, y = energy)) +\n  geom_point(aes(color = listener_id)) +\n  labs(title = \"Comparing Dancing Styles\")\n# Low energy but high danceable would be slower dance songs, both of us prefer high energy dancing music but Kiran definitely goes harder.\n\nggplot(total_audio, aes(x = tempo, y = instrumentalness)) +\n  geom_point(aes(color = listener_id))\n\n# Most danceable tracks, I wish we added artist as well\narrange(total_audio, desc(danceability)) |> \n  select(track.name, listener_id, danceability) |> \n  head()\n```\n:::\n\n\n### Set Up Variables\n\nI will be creating several machine learning models, and use these variables as the start for them all.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(14)\n\n\ntotal_audio <- total_audio |> \n  select(-track.name)\n\n\nsong_split <- initial_split(total_audio)\nsong_test <- testing(song_split)\nsong_train <- training(song_split)\n\n# Preprocessing\n### MAKE SURE TO CHANGE RECIPE NAMES THROUGHOUT\nsong_recipe <- recipe(listener_id ~ ., data = song_train) |> \n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) |> \n  step_normalize(all_numeric(), -all_outcomes()) |> \n  prep()\n\n# Cross Validation to tune parameter\ncv_folds <- song_train |> \n  vfold_cv(v = 5)\n```\n:::\n\n\n### K Nearest Neighbors Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bake\nknn_baked <- bake(song_recipe, song_train)\n\n# Apply recipe to test data\nknn_test <- bake(song_recipe, song_test)\n\n# Specify nearest neighbor model\nknn_spec <- nearest_neighbor(neighbors = 7) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\")\n\n# Workflow\nknn_workflow <- workflow() |> \n  add_model(knn_spec) |> \n  add_recipe(song_recipe)\n\n# Fit resamples\nknn_res <- knn_workflow |> \n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE))\n\n# Check Performance\nknn_res |> collect_metrics()\n\n# Specify: Define model with tuning\n# Should I just do this the first time with spec, or run it again??\nknn_spec_tune <- nearest_neighbor(neighbors = tune()) |> \n  set_engine(\"kknn\") |> \n  set_mode(\"classification\")\n\n# Workflow: Define new workflow\n# Should I just skip the first spec/workflow??\nknn_workflow_tune <- workflow() |> \n  add_model(knn_spec_tune) |> \n  add_recipe(song_recipe)\n\n# Fit workflow on predefined folds and hyperparameters\nknn_cv_fit <- knn_workflow_tune |> \n  tune_grid(\n    cv_folds,\n    # Select other neighbors??\n    grid = data.frame(neighbors = c(1, 5, seq(10, 100, 10))))\n\n# Check performance\nknn_cv_fit |> collect_metrics()\n#Results will show the n averaged over all the folds. Use this to predict the best\n```\n:::\n\n\nPredict!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Workflow: Final\nknn_final_wf <- knn_workflow_tune |> \n  finalize_workflow(select_best(knn_cv_fit, metric = \"accuracy\"))\n\n# Fit: Final\nknn_final_fit <- knn_final_wf |> fit(data = song_train)\n\nknn_final_fit <- knn_final_wf |> last_fit(song_split)\n\nknn_metrics <- knn_final_fit |> collect_metrics()\n```\n:::\n\n\n### Decision Tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# dec tree specification tuned to the optimal parameters\ndec_tree_spec_tune <- decision_tree(\n  cost_complexity = tune(),\n  tree_depth = tune(),\n  min_n = tune()) |>\n   set_engine(\"rpart\") |>\n    set_mode(\"classification\")\n\ndec_tree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          min_n(),\n                          levels = 4)\n\ndoParallel::registerDoParallel() #build trees in parallel\n\ndec_tree_rs <- tune_grid(\n  dec_tree_spec_tune,\n  listener_id ~ .,\n  resamples = cv_folds,\n  grid = dec_tree_grid,\n  metrics = metric_set(accuracy)\n)\n\nautoplot(dec_tree_rs) + theme_light()\n\ndec_final_tree <- finalize_model(dec_tree_spec_tune, \n                             select_best(dec_tree_rs))\n\nfinal_dectree_fit <- last_fit(dec_final_tree, \n                           listener_id ~ ., \n                           song_split)   # does training and testing runs\nfinal_dectree_fit$.predictions\n\ndtree_metrics <- final_dectree_fit |> collect_metrics()\n```\n:::\n\n\n### Bagging\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tune specs\ntree_spec_tune <- bag_tree(\n  mode = \"classification\",\n  cost_complexity = tune(),\n  tree_depth = tune(),\n  min_n = tune()) |> \n  set_engine(\"rpart\", times = 50)\n\n# Define tree grid\ntree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)\n\n# New workflow\nwf_tree_tune <- workflow() |> \n  add_recipe(song_recipe) |> \n  add_model(tree_spec_tune)\n\n\n# Build in parallel\ndoParallel::registerDoParallel()\n\n# # fit model\ntree_rs <- wf_tree_tune |>\n  tune_grid(listener_id ~ .,\n    resamples = cv_folds,\n    grid = tree_grid,\n    metrics = metric_set(accuracy))\n\ntree_rs |> collect_metrics()\n\nfinal_bag <- finalize_workflow(wf_tree_tune, select_best(tree_rs, \"accuracy\")) |> \n  fit(data = song_train)\n\n# Predictions\nbag_pred <- final_bag |> \n  predict(new_data = song_test) |> \n  bind_cols(song_test)\n\n# Output accuracy\nbag_metrics <- bag_pred |> \n  metrics(truth = listener_id, estimate = .pred_class)\n\nbag_metrics\n```\n:::\n\n\n### Random Forest\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Defining validating set\nset.seed(123)\nval_set <- validation_split(song_train, \n                            strata = listener_id, \n                            prop = 0.70)\n\n## Creating Random Forest specification\nrf_spec <-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n## Defining Random Forest workflow\nrf_workflow <- workflow() %>%\n  add_recipe(song_recipe) %>%\n  add_model(rf_spec)\n\n## Build in parallel\ndoParallel::registerDoParallel()\nrf_res <- \n  rf_workflow %>% \n  tune_grid(val_set,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(accuracy))\n\n## Output model metrics\nrf_res %>% collect_metrics()\n\n## Find the best accuracy metric\nrf_res %>% \n  show_best(metric = \"accuracy\")\n\n## Plot results\nautoplot(rf_res)\n\n## Select best Random Forest model\nbest_rf <- select_best(rf_res, \"accuracy\")\nbest_rf\n\n## Output predictions\nrf_res %>% \n  collect_predictions()\n\n## Defining final model while working in parallel\ndoParallel::registerDoParallel()\nlast_rf_model <- \n  rand_forest(mtry = 2, min_n = 3, trees = 1000) %>% \n  set_engine(\"ranger\", importance = \"impurity\") %>% \n  set_mode(\"classification\")\n\n## Updating our workflow\nlast_rf_workflow <- \n  rf_workflow %>% \n  update_model(last_rf_model)\n\n## Updating our model fit\nset.seed(123)\nlast_rf_fit <- \n  last_rf_workflow %>% \n  last_fit(music_split)\n\n## Outputting model metrics\nrandom_forest_metrics <- last_rf_fit %>% \n  collect_metrics()\n\nrandom_forest_metrics\n\n## Outputting the variables that are most important to our model\nlast_rf_fit %>% \n  extract_fit_parsnip() %>% \n  vip::vip(num_features = 12) \n```\n:::\n\n\n### Comparing Metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# # nearest neighbors metrics\nknn_accuracy <- knn_metrics$.estimate[1]\n# \n# # decision tree metrics\ndtree_accuracy <- dtree_metrics$.estimate[1]\n# \n# # bag tree metrics\nbag_accuracy <- bag_metrics$.estimate[1]\n\n# Random Forest metrics\nrf_accuracy <- random_forest_metrics$.estimate[1]\n\n\nmodel_accuracy <- tribble(\n  ~\"model\", ~\"accuracy\",\n  \"KNN\", knn_accuracy,\n  \"Decision Tree\", dtree_accuracy,\n  \"Bagging\", bag_accuracy,\n  \"Random Forest\", rf_accuracy\n)\n\nggplot(data = model_accuracy, aes(x = model, y = accuracy)) +\n         geom_col() +\n  theme_minimal() +\n  labs(title = \"Comparison of Model Accuracy for Spotify Data\")\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}