{
  "hash": "175f53bd9c5a36d7adca745cecdf5b5b",
  "result": {
    "markdown": "---\ntitle: \"Text and Sentiment Analysis\"\ndescription: \"Exploring Text and Sentiment Analysis of Biodiversity Loss Articles\"\nauthor:\n  - name: Erica Dale\n    url: http://ericamarie9016.github.io\n    affiliation: MEDS\n    affiliation-url: http://ucsb-meds.github.io\ndate: 2023-05-11\nformat:\n  html:\n    code-fold: false\n    code-summary: \"Show the code\"\ncode-overflow: wrap\ncode-block-bg: true\ncode-block-border-left: \"#6B5A75\"\ncategories: [MEDS, Text Analysis, R, Tutorial]\ncitation: \n  url: http://ericamarie9016.github.io/2023-05-11-text-analysis\ndraft: TRUE\n---\n\n\nIn this project I analyze the sentiment of biodiversity loss in recent New York Times articles. The COP15 United Nations Biodiversity Conference was in December 2022, so I am particularly interested to see if there has been a change in sentiment before and after this conference. I hope to see that this international conference has brought increased attention towards this subject. To compare, I separate the data into the 5 months prior to the conference, and the 5 months after.\n\n## Setting Up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(jsonlite) #convert results from API queries into R-friendly formats \nlibrary(tidyverse)\nlibrary(tidytext) #text data management and analysis\nlibrary(ggplot2) #plot word frequencies and publication dates\n```\n:::\n\n\nThe collection of articles were gathered from New York Times developer, via (\\<https://developer.nytimes.com/get-started\\>). I created an app with an Article Search to get an API key and used the {jsonlite} package to query the API. Follow the below code to import your own collection of documents, changing the terms and time period for your interest. This initial query may take a long time, but the code will update which page it is retrieving from the API.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAPI_KEY <- 'your token'\nterm1 <- \"biodiversity\" \nterm2 <- \"&loss\"\nbegin_date <- \"20220701\"\nend_date <- \"20230501\"\n\n# Link construction for the API\nbaseurl <- paste0(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\",term1,term2,\"&begin_date=\",begin_date,\"&end_date=\",end_date,\"&facet_filter=true&api-key=\",\"NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\", sep=\"\")\n  \n# Send the request, receive the response, and flatten\nt <- fromJSON(baseurl, flatten = T)\nt <- data.frame(t)\ninitialQuery <- fromJSON(baseurl)\nmaxPages <- round((initialQuery$response$meta$hits[1] / 10) - 1)\npages <- list()\n\nfor(i in 0:maxPages){\n  nytSearch <- fromJSON(paste0(baseurl, \"&page=\", i),\n                        flatten = TRUE) |> \n    data.frame()\n  message(\"Retrieving page \", i)\n  pages[[i+1]] <- nytSearch\n  Sys.sleep(20)\n}\n```\n:::\n\n\nI ran into issues further along the code, and realized the documents did not have the same number of columns. After some sleuthing I realized some of the data frames were missing the column \"response.docs.subsection_name\". This may not be an issue with your query, but the below code will resolve this if it is. The following step, which requires all data frames to have the same column names, is to rbind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npages <- lapply(pages, function(df) {\n  if(ncol(df) == 32) {\n    df |>  mutate(\"response.docs.subsection_name\" = NA)\n  } else {\n    df\n  }\n})\n\n# Combine all the documents\nbiodiversitydf <- do.call(\"rbind\", pages)\n```\n:::\n\n\nI completed the above steps on my computer and saved the results, so here I'll read in my own file to continue. I filter the data frame to before and after the COP-15 Conference in December 2020\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiodiversitydf <- readRDS(\"nyt_biodiversity_df.rds\")\n\n# Need this version for editing, above for knitting\n#biodiversitydf <- readRDS(\"posts/2023-05-11-text-analysis/nyt_biodiversity_df.rds\")\n\n# Filter by the publication date\nPre_COP15 <- biodiversitydf |> \n  filter(as.Date(response.docs.pub_date) < as.Date(\"2022-12-7\"))\nPost_COP15 <- biodiversitydf |> \n  filter(as.Date(response.docs.pub_date) > as.Date(\"2022-12-7\"))\n```\n:::\n\n\n## Exploration\n\nFirst we can review the different types of materials/publications that are covering Biodiversity Loss. In both cases, we see it's news coverage mostly covering this topic. This means those who do not follow the news, or get most of their information through magazines and social media, are not exposed to this information.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# article type\npost_pubs <- Post_COP15 %>% \n  group_by(response.docs.type_of_material) %>%\n  summarize(count=n()) %>%\n  mutate(percent = (count / sum(count))*100) %>%\n  ggplot() +\n  geom_bar(aes(y=percent, x=response.docs.type_of_material), stat = \"identity\") + coord_flip() +\n  theme_minimal() +\n  labs(x = \"Material Type\", y = \"Percent\", title = \"Post-COP15 Publication Types\") +\n  scale_fill_discrete(name = \"Types of Material\") + \n  theme(legend.position = \"none\")\n\npre_pubs <- Pre_COP15 %>% \n  group_by(response.docs.type_of_material) %>%\n  summarize(count=n()) %>%\n  mutate(percent = (count / sum(count))*100) %>%\n  ggplot() +\n  geom_bar(aes(y=percent, x=response.docs.type_of_material), stat = \"identity\") + coord_flip() +\n  theme_minimal()  +\n  labs(x = \"Material Type\", y = \"Percent\", title = \"Pre-COP15 Publication Types\") + \n  theme(legend.position = \"none\")\n\ngridExtra::grid.arrange(pre_pubs, post_pubs)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nNext I explore the sentiment by pulling out paragraphs. The sixth column is called \"response.doc.lead_paragraph\" which has the paragraph text saved. This next step pulls out each word in the paragraph column, calling the words tokens and putting each in their own row.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npre_paragraph <- names(Pre_COP15)[6] \n\npre_paragraph_tokenized <- Pre_COP15 |> \n  unnest_tokens(word, pre_paragraph)\n\n# Explore word column\nhead(pre_paragraph_tokenized[,\"word\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"nashville\" \"as\"        \"a\"         \"young\"     \"college\"   \"student\"  \n```\n:::\n\n```{.r .cell-code}\npost_paragraph <- names(Post_COP15)[6] #\n\npost_paragraph_tokenized <- Post_COP15 |> \n  unnest_tokens(word, post_paragraph)\n\n# Explore word column\nhead(post_paragraph_tokenized[,\"word\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"to\"     \"learn\"  \"more\"   \"about\"  \"humans\" \"a\"     \n```\n:::\n:::\n\n\nThere is now a column called \"word\" that has each word from each document. There are a lot of words that build sentences without having stand-alone meaning that would be useful for this context. So next we are going to remove what's called \"stop words\". This vector of words is available through the package {tidytexts}. An anti-join is used to remove any rows that have these words.\n\nAdding stop words and other transformations to the corpus:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npre_paragraph_tokenized <- pre_paragraph_tokenized |> \n  anti_join(stop_words)\n\npost_paragraph_tokenized <- post_paragraph_tokenized |> \n  anti_join(stop_words)\n```\n:::\n\n\nWord frequencies:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npre_graph_paragraph <- pre_paragraph_tokenized |> \n  count(word, sort = TRUE) |> \n  filter(n > 7) |> \n  mutate(word = reorder(word, n)) |> \n  ggplot(aes(n, word)) +\n  geom_col()  +\n  labs(x = \"Word Count\", y = NULL, title = \"Top Words Before the COP15 Meeting\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 15, face = \"bold\", margin = margin(b = 10)),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank())\n\npost_graph_paragraph <- post_paragraph_tokenized |> \n  count(word, sort = TRUE) |> \n  filter(n > 6) |> \n  mutate(word = reorder(word, n)) |> \n  ggplot(aes(n, word)) +\n  geom_col()  +\n  labs(x = \"Word Count\", y = NULL, title = \"Top Words After the COP15 Meeting\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 15, face = \"bold\", margin = margin(b = 10)),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank())\n\ngridExtra::grid.arrange(pre_graph_paragraph, post_graph_paragraph)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nI want to explore the headlines, as the words in these can catch attention and direct feelings and interest in the article. The 20th column is \"response.docs.headline.main\" and we are going to repeat the stop words again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npre_headline <- names(Pre_COP15)[20]\n\npre_headline_tokenized <- Pre_COP15 |> \n  unnest_tokens(word, pre_headline)\n\npre_headline_tokenized <- pre_headline_tokenized |> \n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n\n```{.r .cell-code}\npost_headline <- names(Post_COP15)[20]\n\npost_headline_tokenized <- Post_COP15 |> \n  unnest_tokens(word, post_headline)\n\npost_headline_tokenized <- post_headline_tokenized |> \n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(word)`\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npre_graph_headline <- pre_headline_tokenized |> \n  count(word, sort = TRUE) |> \n  filter(n > 3) |> \n  mutate(word = reorder(word, n)) |> \n  ggplot(aes(n, word)) +\n  geom_col()  +\n  labs(x = \"Word Count\", y = NULL, title = \"Headlines Prior to COP-15\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 15, face = \"bold\", margin = margin(b = 10)),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank())\n\n\npost_graph_headline <- post_headline_tokenized |> \n  count(word, sort = TRUE) |> \n  filter(n > 2) |> \n  mutate(word = reorder(word, n)) |> \n  ggplot(aes(n, word)) +\n  geom_col()  +\n  labs(x = \"Word Count\", y = NULL, title = \"Headlines After COP-15\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 15, face = \"bold\", margin = margin(b = 10)),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank())\n\ngridExtra::grid.arrange(pre_graph_headline, post_graph_headline)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Sentiment\n\nNow we will take the previous exploration and further analyze for sentiment in the text. I begin with taking the tokenized words from leading paragraph, already have separated out stop words previously. Using the sentiments from bing, I am adding a numeric binary value 0/1 to the words for positive/negative connotation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbing_sent <- get_sentiments('bing')\n\n\npre_sent_words <- pre_paragraph_tokenized |> \n  inner_join(bing_sent, by = \"word\") |>     # inner join sentiment words\n  mutate(sent_num = case_when(               # turn sentiment into numerical form\n    sentiment == 'negative' ~ -1,\n    sentiment == 'positive' ~ 1\n  ) )\n\n\npost_sent_words <- post_paragraph_tokenized |> \n  inner_join(bing_sent, by = \"word\") |>     # inner join sentiment words\n  mutate(sent_num = case_when(               # turn sentiment into numerical form\n    sentiment == 'negative' ~ -1,\n    sentiment == 'positive' ~ 1\n  ) )\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}